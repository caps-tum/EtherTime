
%\section{Time Synchronization, PTP \& Related Work}
\section{Protocols Surveyed} % \& Related Work}
\label{sec:background}

\todo{Check out related paper suggestions by Arpan}
%Over the years,
Several time synchronization protocols, algorithms, and solutions
%across computer networks
have been explored~\cite{ntpv4-spec, sntp-spec, linuxptp, time-protocol-flooding, time-protocol-pulsesync, white-rabbit, time-protocol-wsn, time-protocol-low-power}.
%In today's most widely deployed network stack for general purpose computing, IP,
Two protocols have established themselves as the unofficial standard:
%for time synchronization:
Network Time Protocol (NTP)~\cite{ntpv4-spec} for wide area networks %(WANs)
and Precision Time Protocol (PTP)~\cite{ptp-spec} for local area networks %(LANs)
(which require a greater degree of precision).
Both protocols have standardized specifications and multiple implementations
available at the time of writing.

Since we are interested in evaluating time synchronization for distributed
embedded systems,
we focus on PTP and derivatives,
which are designed for controlled environments
%(such as fault-tolerant industrial control networks)
and have widespread adoption across multiple domains,
%general computing and
such as industrial control networks, robotics, and telecommunications.
PTP also serves as a foundation for technologies such as
IEEE Time Sensitive Networking~\cite{??}.

\todo{I have commented out what looked like the single related work paragraph. It seemed out of place between two PTP paragraphs, which provide the necessary background for this work. We can move the related work to end if it is going to be this short.}
%Clock synchronization protocols have been proposed for many different applications including packet-switched networks~\cite{sptp, white-rabbit, time-protocol-flooding, time-protocol-low-power} and their strengths and weaknesses have been evaluated analytically~\cite{clock-synchronization-packet-switched-networks}. Due to the need for dependability, multiple studies have compared how time protocols can fail~\cite{ptp-failures}, how fault tolerance and redundancy can be offered~\cite{fault-tolerant-clock-synchronization-distributed-systems}, and how timing protocols might be attacked by malicious actors~\cite{ptp-internal-attacks, byzantine-ptp}. However, we find that there is a lack of literature collecting empirical findings across implementations and protocols, with an early study~\cite{ntp-vs-ptp} from 2006 focusing on technologies that were available at the time and more recent studies~\cite{time-enough} not incorporating fault-tolerance as a central aspect.

We surveyed available PTP implementations and found that while there are plenty of commercialized options,
the number of viable freely available implementations is rather limited.
\cref{tab:vendors} summarizes the results. % of our survey.
\todo{What do you mean by viable?}
We excluded four vendors from our evaluation:
\textbf{(i)}~OpenPTP~\cite{openptp} is currently unmaintained; its last activity was more than ten years ago, and it has since been commercialized.
\textbf{(ii)}~Timebeat~\cite{timebeat} relies on heavy-weight infrastructure
(specifically the Elasticsearch/Logstash/Kibana stack) making it unsuitable for embedded applications.
\textbf{(iii)}~PPSI~\cite{ppsi} has stability issues caused by buffer overruns;
we have filed a bug report\cite{ppsi-bug-report} in this regard.\todo{@arpan: cited as personal communication: submitted via email because no public issue tracker is available}
\textbf{(iv)}~We also reviewed White rabbit~\cite{white-rabbit},
an open extension to PTP for sub-nanosecond timing.
It requires highly specialized hardware
(vendor-specific synchronous Ethernet switches and NICs with \emph{synchronization} support),
which makes it less suitable for COTS embedded systems.

This left us with four suitable vendors: PTPd, LinuxPTP, SPTP and Chrony. %(summarized in Table~\ref{tab:vendors}).
\textbf{(a)}~PTPd~\cite{ptpd-manpage} %is a traditional implementation of the PTP protocol that, despite being less
has been less maintained in recent years~\cite{ptpd-maintainers} and lacks
modern features such as hardware timestamping;
but it has spawned a variety of derivates (also commercial) and is still being
deployed as per the Debian package tracker~\cite{debian-popularity-contest}
(perhaps due to its relative simplicity and wider support of non-Linux UNIX systems).
\textbf{(b)}~LinuxPTP~\cite{linuxptp-homepage} is the most deployed open-source
PTP solution on Debian.
Its goal is to provide robustness while tightly integrating with Linux, taking advantage of the kernel and hardware features provided to improve
synchronization accuracy.
\textbf{(c)}~SPTP~\cite{facebook-sptp} is a novel time synchronization protocol
%consisting of the PTP4U server~\cite{??} and the SPTP client,
developed at Meta due to difficulties they encountered with deploying PTP.
It claims to perform on par with PTP in terms of time synchronization,
%a comparable level of time-synchronization performance
while consumes less resources and offering higher resilience.
\textbf{(d)}~Finally, we include Chrony~\cite{??}, the state-of-the-art NTP implementation.
%as a reference.
It is by far the most feature-complete implementation of time synchronization
among the ones we evaluated.
%and is far more widely deployed than PTP~\cite{debian-popularity-contest},
%likely due to its applicability to wide area networks.
As a general-purpose protocol, %for time synchronization,
it serves as a baseline for other PTP implementations.
%it serves as a baseline for the other implementations so that we can compare PTP implementation performance to the performance of general-purpose time synchronization.


\begin{table}
    \caption{Time-Synchronization Protocols Surveyed and Evaluated (\checkmark)}
    \begin{tabular}{lll}
    \toprule
        & \textbf{Vendor (Version)} & \textbf{Remarks} \\
    \midrule
        \checkmark & PTPd (2.3.1)     & Established implementation with derivatives\\
        \checkmark & LinuxPTP (3.1.1) & Advanced capabilities but specific to Linux\\
        \checkmark & SPTP (b27bdba)   & Meta's custom time synchronization protocol\\
        \checkmark & Chrony (4.3)     & State-of-the-art NTP server/client\\
        --         & OpenPTP (r2)     & Unmaintained for more than 10 years \\
        --         & Timebeat (2.0.7) & Unsuitable for embedded systems \\
        --         & PPSI (6.1)       & Critical bug found \\
        --         & White Rabbit (-) & Requires specialized hardware \\
    \bottomrule
    \end{tabular}
    \label{tab:vendors}
\end{table}

%\begin{table}
%    \caption{Surveyed Time-Synchronization Protocols}
%    \begin{tabular}{lcc@{\,}rl}
%        Vendor & Protocol & \multicolumn{2}{c}{Included} & Notes\\\hline
%        PTPd & PTP & \checkmark & 2.3.1 & Established implementation with derivatives\\
%        LinuxPTP & PTP & \checkmark & 3.1.1 & Advanced capabilities but specific to Linux\\
%        SPTP\textsuperscript{*} & Custom & \checkmark & b27bdba & Meta's custom time protocol\\
%        Chrony & NTP & \checkmark & 4.3 & Widespread NTP server/client\\
%        OpenPTP & PTP & $\times$ & r2 & Unmaintained \\
%        Timebeat & PTP & $\times$ & 2.0.7 & Unsuitable for embedded \\
%        PPSI & PTP & $\times$ & 6.1 & Critical bug \\
%        White Rabbit & Custom & $\times$ & - & Specialized hardware requirements\\
%    \end{tabular}
%    \label{tab:vendors}
%    \textsuperscript{*} Note that we had to patch 32-bit support into SPTP for our ARMv7 boards.
%\end{table}

%\subsection{PTP -- Background and Architecture}
\section{PTP Overview}
\label{sec:ptp-overview}
%PTP operates across a network and has
There are two types of endpoints: master nodes and slave nodes.
The goal is to synchronize the local clocks on slaves nodes with the master's
clock.
Obtaining a good clock signal on the master is out of scope for PTP,
as it is assumed that the master already has a high-quality external clock source,
e.g., a GPS signal, an atomic clock, or another PTP domain.

The master node periodically sends a synchronization signal to all slave nodes
to distribute the current master time.
Each slave uses the signal in combination with a path delay estimate
(obtained using a separate delay request/response mechanism)
to predict its local clock offset with respect to the master clock.
The clock offset is subsequently used to update and synchronize the local clock
with the master's clock.

%PTP operates across a network and has two types of endpoints: master nodes and slave nodes. A PTP master provides signals to the slaves (Figure \ref{fig:ptp-architecture}), which each use the synchronization signal in combination with a path delay estimate to determine an estimate of the local clock offset to the master clock, which is subsequently used to discipline the local clock to keep it synchronized with the master's clock. While the actual protocol is slightly more complex and includes additional messages for synchronization leases and state management, these two core signals/message types are sufficient in principle to synchronize the system clocks. Note that obtaining a good clock signal on the master is considered out of scope for PTP, it is assumed that the master already has a high-quality clock source for the current time -- preferably an external source like an atomic clock or a GPS signal, but the master clock can also be obtained from e.g. a different PTP domain.

%\begin{figure}
%    \begin{tikzpicture}[client/.style={draw}, message/.style={midway, above, sloped, inner sep=0mm}, on grid]
%        \node[client] (master) {PTP Master};
%        \node[client, above=of master] (clock) {\faClock[regular]};
%        \draw[thick, dotted] (master) -- (clock);
%
%        \node[client, right=1.75cm of master] (switch) {\faNetworkWired};
%        \draw[-Stealth] (master) -- (switch);
%
%        \foreach \i in {1,...,3}{
%            \node[client] (slave-\i) at (3.5, \i - 2) {PTP Slave};
%            \draw[-Stealth] (switch) -- (slave-\i.west);
%        }
%
%        \begin{scope}[xshift=5.5cm, yshift=2cm]
%            \node (master) at (0,0) {Master};
%            \node (slave) at (2,0) {Slave};
%
%            \draw[-] (master) -- ++(0, -3.75);
%            \draw[-] (slave) -- ++(0, -3.75);
%
%            \foreach \i in {0.5, 1, 1.5}{
%                \draw[-Stealth] (0, -\i) -- ++(2, -0.5) node[message] {Sync};
%            }
%
%            \draw[-Stealth] (2, -2.5) -- ++(-2, -0.5) node[message] {Delay Req.} -- ++(2, -0.5) node[message, below, inner sep=0.5mm] {Delay Resp.};
%        \end{scope}
%    \end{tikzpicture}
%    \caption{
%        A PTP master provides a synchronization signal to a number of slaves so that they can keep their local clock synchronized to the master's clock (left). The clock synchronization relies on two types of signals (right): a periodic synchronization signal to distribute the current master time and the delay request/response to estimate the propagation delay.
%    }
%    \label{fig:ptp-architecture}
%\end{figure}

%\subsection{PTP Profiles}

\todo{Commented out the PTP profile paragraph. I think you should simply state that we use the default vendor profiles wherever possible, when you first discuss the evaluation.}
%PTP is built to be configurable, and settings include anything from the
%underlying transport (unicast/multicast packet switching), the delay mechanism
%to use (end-to-end or peer-to-peer), message frequencies and leases, as well as
%rules to update the clock.
%%To reduce the complexity of configuring PTP, several so-called profiles are
%%available that provide default settings for the specific use-case, such as
%%general-purpose or ITU telecoms.
%In our evaluation, we use each vendor's default profile, as this profile is a
%widely-applicable general-purpose profile that does not require
%special configuration, and is thus likely to be deployed in many different
%contexts.

%PTP is built to be configurable, and settings include anything from the underlying transport (unicast/multicast packet switching), the delay mechanism to use (end-to-end or peer-to-peer), message frequencies and leases, as well as rules to discipline the clock. To reduce the complexity of configuring PTP, several so-called profiles are available that provide default settings for the specific use-case, such as general-purpose or ITU telecoms. To conduct our evaluation, we examine each vendor's default profile, as this profile is a widely-applicable general-purpose profile that does not require special configuration, and is thus likely to be deployed in many different contexts.


\subsection{PTP Lifecycle}


\begin{figure*}
    \newcommand{\intervalAnnotation}[4]{
        \draw[Bar-Bar] ([yshift=#3, xshift=-0.1cm]#1.south west) -- ([yshift=#3, xshift=0.1cm]#2.south east) node[midway, below] {#4};
    }
    \newcommand{\intervalAnnotationAbove}[4]{
            \draw[Bar-Bar] ([yshift=#3, xshift=-0.1cm]#1.north west) -- ([yshift=#3, xshift=0.1cm]#2.north east) node[midway, above] {#4};
    }

    \begin{tikzpicture}[
        start chain=stages going right,
        stage/.style={
            draw, on chain=stages,
        },
        connected/.style={
            every on chain/.style={join=by -Stealth}
        },
        text depth=0cm,
        ]
        \node[stage] (discovery) {Discovery};
        \draw[-Stealth] (discovery.west) -| ++(-0.2, 0.5) -| ($(discovery.north west) + (0.2, 0)$);

        \node[stage] (bmca) {Best Master Clock Algorithm};
        \draw[{Straight Barb[reversed]}-{Straight Barb[]}, dotted] (discovery) -- (bmca) node[midway, align=center] {\footnotesize{} Peer\\\footnotesize{} Data};
        \node[stage, connected] (calibrate) {Calibration};
        \node[stage, connected] (step) {Clock Step};
        \node[stage, connected] (slew) {Clock Slew};
        \node[stage, connected] (maintain) {Stable};

        \foreach \linkNode in {step,slew,maintain}{
            \draw[-Stealth] (\linkNode) -- ++(0, 0.5) -| (calibrate);
            \draw[-Stealth] (\linkNode) -- ++(0, 0.5) -| (bmca);
            \draw[-Stealth] (calibrate) -- ++(0, -0.5) -| (\linkNode);
        }

        \intervalAnnotation{step}{slew}{-0.5cm}{Converging}
        \intervalAnnotation{maintain}{maintain}{-0.5cm}{Synchronized}
        \intervalAnnotationAbove{calibrate}{maintain}{0.5cm}{Clock Synchronization}
        \intervalAnnotationAbove{discovery}{bmca}{0.5cm}{Network Domain Logic}

    \end{tikzpicture}
    \caption{Different stages in the PTP lifecycle that a PTP slave traverses while synchronizing its clock. For stability of the system clock, some state transitions are typically restricted.}
    \label{fig:ptp-lifecycle}
\end{figure*}

PTP clients traverse multiple stages in a lifecycle to synchronize their clock (Figure~\ref{fig:ptp-lifecycle}). At any point, unexpected conditions such as loss of connectivity can lead PTP to return to an earlier stage in the lifecycle, potentially changing the operating conditions.

\begin{enumerate}[label=S\arabic*.]
    \item \textbf{Discovery} is the stage where remote clocks are identified, usually via periodic multicast announcements. Discovered clients are collected into a PTP domain.
    \item \textbf{Best Master Clock Algorithm (BMCA)} is the standardized algorithm used for each peer to determine whether it should become a master clock or a slave, which can be configured using priorities and clock properties~\cite{bmca-deep-dive}. By the end of this phase, each peer will deterministically become either a master clock or a slave. Slaves proceed to connect to, and negotiate with, the master clock.
    \item \textbf{Calibration} is a brief phase where no local clock modifications are made yet to allow the estimate of the offset to increase in precision through multiple synchronization intervals and path delay estimates.
    \item \textbf{Clock Step} Using a reasonably precise estimate of the offset, an attempt is made to directly reset the slave's local time. Since this is expected to break applications (violation of invariants I2 and potentially I1), it is only used where clock-slew is infeasible (e.g. offset~$>1$~second) and many profiles restrict clock steps to once during the initial synchronization.
    \item \textbf{Clock Slew} is the second phase of convergence where time is made to pass slightly slower or slightly quicker via kernel parameters to fine-tune, thus further reducing offset. The rate of change is limited via software and configuration parameters, e.g. to 500 parts per million/0.05\% in the Linux kernel~\cite{adjtimex}. These limits ensure that the clock signal does not drift too rapidly, but can make convergence slow -- 0.05\% implies that correcting 1 second of offset takes at least 33 minutes.
    \item \textbf{Stable} is the phase where the clock offset cannot be reduced further, and therefore the clocks are as synchronized as they can get. In this phase, further clock slew is happening to correct clock frequency differences and slight variations still occur due to imperfections in offset estimation, but at this point the clock signal is ready to be consumed by applications.
\end{enumerate}

For calculating metrics on the quality of the clock synchronization, it is important to consider what should be measured. Often, comparisons are only meaningful when PTP is in comparable states (stable synchronization is usually of most interest), but ensuring this is the case is not always trivial, especially across vendors.
In terms of resilience, it is also important to determine what causes clocks to become unstable.
%pre-process the data to exclude data during the convergence phase and only consider observations from the synchronized/stable state. We apply this processing offline by determining the rate of change of the estimated offset's sign -- when frequent switches from positive to negative happen then the algorithm is no longer sure whether its own clock is early or tardy, thus we are close to the optimum synchronization.


\subsection{Synchronization Performance}
It is generally understood that network clock synchronization accuracy is a function of the signal propagation delay and its variance~\cite{managin-pdv-for-ptp}. A naive approach to clock synchronization that just sends a timestamp from the master to the slave would always be off by the propagation delay, but PTP uses path delay estimation compensate for the propagation delay thus increasing the accuracy. In an ideal world where there is no packet delay variation, the propagation delay could be mitigated entirely, but in reality we have multiple software and hardware components, like the kernel, network stack and network hardware, that each introduce latency variability, which in turn worsens the performance of the delay compensation. Effects, such as asymmetric latencies caused by uneven loads, that cannot easily be compensated for further reduce the synchronization accuracy. Thus, limiting the packet delay variation becomes a primary concern when tuning for precision and dependability.

\subsection{Hardware Support}

\begin{figure}
    \centering
    \newcommand{\timestampClock}[1][100]{\textcolor{black!#1}{\faClock[regular]}}
    \small

    \begin{tikzpicture}[
        start chain=components going below,
        start chain=components2 going below,
        component block/.style={draw, minimum height=0.6cm},
        node distance=0cm,
    ]

            \begin{scope}[name prefix=stack1-, component/.style={component block, text width=3.25cm}]
                \node[on chain=components, component] at (0, 0) (PTP) {PTP Client \hfill \smash{\raisebox{-0.2ex}{\rotatebox{45}{\faStamp{}}}} \timestampClock{} \faEnvelope[regular]};
                \node[on chain=components, component] (Kernel) {Kernel \hfill \timestampClock[60] \faEnvelope[regular]};
                \node[on chain=components, component] (IP-Stack) {IP Stack \hfill \timestampClock[40] \faEnvelope[regular]};
                \node[on chain=components, component] (Hardware Queue) {Hardware Queue \hfill \timestampClock[30] \faEnvelope[regular]};
                \node[on chain=components, component] (NIC) {NIC \faEthernet \hfill \timestampClock[20] \faEnvelope[regular]};

                \node[above=of PTP] (title) {Software PTP};
            \end{scope}

            \begin{scope}[name prefix=stack2-, component/.style={component block, text width=3.25cm}]
                \node[on chain=components2, component] at (5.25, 0) (PTP) {\faEnvelope[regular] \hfill PTP Client};
                \node[on chain=components2, component] (Kernel) {\faEnvelope[regular] \hfill Kernel};
                \node[on chain=components2, component] (IP-Stack) {\faEnvelope[regular] \hfill IP Stack };
                \node[on chain=components2, component] (Hardware Queue) {\faEnvelope[regular] \hfill Hardware Queue};
                \node[on chain=components2, component] (NIC) {\faEnvelope[regular] \faClock[regular] \smash{\raisebox{1.2ex}{\rotatebox{-45}{\faStamp{}}}}  \hfill \faEthernet{} NIC};

                \node[above=of PTP] (title) {Hardware PTP};
            \end{scope}

            \draw[Stealth-Stealth, very thick] (stack1-NIC) -- (stack2-NIC) node[midway, draw, fill=white] (network) {\faNetworkWired};

            \node[draw, dashed, above=0.25cm of network] (network-annotation) {\faClock[regular] $\rightarrow$ \faHistory{}\,\textsuperscript{\textbf{*}}};
            \draw[dotted] (network.north west) -- (network-annotation.south west);
            \draw[dotted] (network.north east) -- (network-annotation.south east);

    \end{tikzpicture}
    \caption{
        Timestamping for PTP messages when using software timestamping (left) and hardware timestamping (right). For software timestamping, the timestamp is generated inside the PTP client and traverses many layers in both egress and ingress, causing additional path delay and delay variation. With hardware timestamping, a timestamp is only added to the message just before it is written to wire by the NIC, thus ensuring a more up-to-date timestamp is sent to the network.\\
        \textsuperscript{\textbf{*}}Network hardware (switches, routers, etc.) also introduces queuing delays. Special PTP-aware hardware can compensate for its own delays to further improve timestamping quality.
    }
    \label{fig:ptp-sw-hw}
\end{figure}

Since delay variation is a primary concern, techniques have been developed to reduce the variability. While PTP can run entirely in software, the path that packets need to traverse between two PTP clients not only includes several hardware components but also some software layers (Figure~\ref{fig:ptp-sw-hw} left). Each component along the path introduces latency and packet delay variation, deteriorating the signal quality. With the appropriate hardware support, message timestamps can instead be generated directly by the NIC driver/hardware (Figure~\ref{fig:ptp-sw-hw} right), which ensures that the timestamp is not affected by the layers above it. This increases the clock synchronization's resilience against interference from adverse conditions, such as high network-, CPU- or kernel load.

However, hardware timestamping alone does not guarantee a high level of dependability. In order to determine what can go wrong, we need to take a look at what constraints the timing system needs to observe and what might lead to potential failures.
