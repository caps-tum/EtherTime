
\section{Time Synchronization, PTP \& Related Work}

Over the years, many different protocols, algorithms and solutions to time synchronization across computer networks have been explored~\cite{ntpv4-spec, sntp-spec, linuxptp, time-protocol-flooding, time-protocol-pulsesync, white-rabbit, time-protocol-wsn, time-protocol-low-power}. In today's most widely deployed network stack for general purpose computing, IP, two protocols have established themselves as standard for time synchronization: the Network Time Protocol (NTP)~\cite{ntpv4-spec} for Wide Area Networks (WANs) and the Precision Time Protocol (PTP)~\cite{ptp-spec} for Local Area Networks (LANs) that require a greater degree of time synchronization precision. Both protocols are standardized and each have multiple implementations available at the time of writing.

Because we are interested in evaluating time synchronization for dependable embedded systems, we will focus on PTP and derivatives as they are designed to be deployed in controlled environments such as fault-tolerant industrial control networks. PTP has wide-spread adoption in general computing and telecommunications, and also serves as a foundation for technologies such as IEEE Time Sensitive Networking (TSN).

Clock synchronization protocols have been proposed for many different applications including packet-switched networks~\cite{sptp, white-rabbit, time-protocol-flooding, time-protocol-low-power} and their strengths and weaknesses have been evaluated analytically~\cite{clock-synchronization-packet-switched-networks}. Due to the need for dependability, multiple studies have compared how time protocols can fail~\cite{ptp-failures}, how fault tolerance and redundancy can be offered~\cite{fault-tolerant-clock-synchronization-distributed-systems}, and how timing protocols might be attacked by malicious actors~\cite{ptp-internal-attacks, byzantine-ptp}. However, we find that there is a lack of literature collecting empirical findings across implementations and protocols, with an early study~\cite{ntp-vs-ptp} from 2006 focusing on technologies that were available at the time and more recent studies~\cite{time-enough} not incorporating fault-tolerance as a central aspect.

We surveyed available PTP implementations and found that while there are plenty of commercial products available, the number of viable freely available implementations is rather limited. We excluded three vendors from our evaluation: OpenPTP~\cite{openptp} due to the unmaintained state of the project (last activity $>10$ years ago, the solution has since been commercialized), Timebeat~\cite{timebeat} due to the reliance on heavy-weight infrastructure (specifically the Elasticsearch/Logstash/Kibana stack) making it unsuitable for embedded applications, and PPSI~\cite{ppsi} due to stability issues caused by buffer overruns (bug report\todo{@arpan: cannot cite like requested: submitted via email because no public issue tracker is available} has been filed), respectively. White rabbit~\cite{white-rabbit}, an open extension to PTP for ultra-precise (sub-nanosecond) timing was also reviewed, but the highly specialized hardware required (White Rabbit-capable synchronous Ethernet switches and NICs with \emph{synctonization} support) makes it less suitable for embedded systems.

This left us with four vendors suitable for our analysis: PTPd, LinuxPTP, SPTP and Chrony (summarized in Table~\ref{tbl:vendors}).
PTPd is a traditional implementation of the PTP protocol that, despite being less maintained in recent years~\cite{ptpd-maintainers} and lacking modern features such as hardware timestamping~\cite{ptpd-manpage}, spawned a variety of derivates (also commercial) and is still being deployed according to the Debian package tracker~\cite{debian-popularity-contest}, perhaps due to its relative simplicity and wider support of non-Linux UNIX systems.
 LinuxPTP is the most deployed~\cite{debian-popularity-contest} open-source PTP solution on Debian, with the stated goals of providing robustness while integrating tightly with Linux and taking advantage of the kernel and hardware features provided to improve synchronization accuracy~\cite{linuxptp-homepage}.
SPTP, a novel time-synchronization protocol consisting of the PTP4U server and the SPTP client described in a 2023 publication by Meta employees~\cite{facebook-sptp}, was developed at Meta due to difficulties they encountered with deploying PTP. It claims to support a comparable level of time-synchronization performance while offering lower resource consumption and better resilience~\cite{facebook-sptp}.
Finally, Chrony is included as a reference representing the state of the art implementation of the Network Time Protocol (NTP)\todo{cite}. It is by far the most feature-complete time-synchronization implementation of the ones we evaluated and is far more widely deployed than PTP~\cite{debian-popularity-contest}, likely due to its applicability to wide area networks. It serves as a baseline for the other implementations so that we can compare PTP implementation performance to the performance of general-purpuse time synchronization.


\begin{table}
    \caption{Surveyed Time-Synchronization Protocols}
    \begin{tabular}{lcc@{\,}rl}
        Vendor & Protocol & \multicolumn{2}{c}{Included} & Notes\\\hline
        PTPd & PTP & \checkmark & 2.3.1 & Established implementation with derivatives\\
        LinuxPTP & PTP & \checkmark & 3.1.1 & Advanced capabilities but specific to Linux\\
        SPTP\textsuperscript{*} & Custom & \checkmark & b27bdba & Meta's custom time protocol\\
        Chrony & NTP & \checkmark & 4.3 & Widespread NTP server/client\\
        OpenPTP & PTP & $\times$ & r2 & Unmaintained \\
        Timebeat & PTP & $\times$ & 2.0.7 & Unsuitable for embedded \\
        PPSI & PTP & $\times$ & 6.1 & Critical bug \\
        White Rabbit & Custom & $\times$ & - & Specialized hardware requirements\\
    \end{tabular}
    \label{tbl:vendors}
    \textsuperscript{*} Note that we had to patch 32-bit support into SPTP for our ARMv7 boards.
\end{table}

To the best of our knowledge, we provide the first empirical evaluation of multiple Ethernet-based time synchronization protocols and implementations across several embedded hardware testbeds with an explicit focus on dependability and fault-tolerance. We also make the data collection and analysis tooling available open-source\todo{anonymize} to enable future studies to generate comparable results.

\subsection{PTP -- Background and Architecture}

PTP operates across a network and has two types of endpoints: master nodes and slave nodes. A PTP master provides signals to the slaves (Figure \ref{fig:ptp-architecture}), which each use the synchronization signal in combination with a path delay estimate to determine an estimate of the local clock offset to the master clock, which is subsequently used to discipline the local clock to keep it synchronized with the master's clock. While the actual protocol is slightly more complex and includes additional messages for synchronization leases and state management, these two core signals/message types are sufficient in principle to synchronize the system clocks. Note that obtaining a good clock signal on the master is considered out of scope for PTP, it is assumed that the master already has a high-quality clock source for the current time -- preferably an external source like an atomic clock or a GPS signal, but the master clock can also be obtained from e.g. a different PTP domain.

\begin{figure}
    \begin{tikzpicture}[client/.style={draw}, message/.style={midway, above, sloped, inner sep=0mm}, on grid]
        \node[client] (master) {PTP Master};
        \node[client, above=of master] (clock) {\faClock[regular]};
        \draw[thick, dotted] (master) -- (clock);

        \node[client, right=1.75cm of master] (switch) {\faNetworkWired};
        \draw[-Stealth] (master) -- (switch);

        \foreach \i in {1,...,3}{
            \node[client] (slave-\i) at (3.5, \i - 2) {PTP Slave};
            \draw[-Stealth] (switch) -- (slave-\i.west);
        }

        \begin{scope}[xshift=5.5cm, yshift=2cm]
            \node (master) at (0,0) {Master};
            \node (slave) at (2,0) {Slave};

            \draw[-] (master) -- ++(0, -3.75);
            \draw[-] (slave) -- ++(0, -3.75);

            \foreach \i in {0.5, 1, 1.5}{
                \draw[-Stealth] (0, -\i) -- ++(2, -0.5) node[message] {Sync};
            }

            \draw[-Stealth] (2, -2.5) -- ++(-2, -0.5) node[message] {Delay Req.} -- ++(2, -0.5) node[message, below, inner sep=0.5mm] {Delay Resp.};
        \end{scope}
    \end{tikzpicture}
    \caption{
        A PTP master provides a synchronization signal to a number of slaves so that they can keep their local clock synchronized to the master's clock (left). The clock synchronization relies on two types of signals (right): a periodic synchronization signal to distribute the current master time and the delay request/response to estimate the propagation delay.
    }
    \label{fig:ptp-architecture}
\end{figure}

\subsection{PTP Profiles}
PTP is built to be configurable, and settings include anything from the underlying transport (unicast/multicast packet switching), the delay mechanism to use (end-to-end or peer-to-peer), message frequencies and leases, as well as rules to discipline the clock. To reduce the complexity of configuring PTP, several so-called profiles are available that provide default settings for the specific use-case, such as general-purpose or ITU telecoms. To conduct our evaluation, we examine each vendor's default profile, as this profile is a widely-applicable general-purpose profile that does not require special configuration, and is thus likely to be deployed in many different contexts.


\subsection{PTP Lifecycle}


\begin{figure*}
    \newcommand{\intervalAnnotation}[4]{
        \draw[Bar-Bar] ([yshift=#3, xshift=-0.1cm]#1.south west) -- ([yshift=#3, xshift=0.1cm]#2.south east) node[midway, below] {#4};
    }
    \newcommand{\intervalAnnotationAbove}[4]{
            \draw[Bar-Bar] ([yshift=#3, xshift=-0.1cm]#1.north west) -- ([yshift=#3, xshift=0.1cm]#2.north east) node[midway, above] {#4};
    }

    \begin{tikzpicture}[
        start chain=stages going right,
        stage/.style={
            draw, on chain=stages,
            every on chain/.style={join}, every join/.style={-Stealth}
        },
        text depth=0cm,
        ]
        \node[stage] (discovery) {Discovery};
        \node[stage] (bmca) {Best Master Clock Algorithm};
        \node[stage] (calibrate) {Calibration};
        \node[stage] (step) {Clock Step};
        \node[stage] (slew) {Clock Slew};
        \node[stage] (maintain) {Stable};

        \intervalAnnotation{step}{slew}{-0.25cm}{Converging}
        \intervalAnnotation{maintain}{maintain}{-0.25cm}{Synchronized}
        \intervalAnnotationAbove{calibrate}{maintain}{0.25cm}{Slave bound to Grandmaster}
        \intervalAnnotationAbove{discovery}{bmca}{0.25cm}{Peer}

    \end{tikzpicture}
    \caption{Different stages in the PTP lifecycle that a PTP slave traverses while synchronizing its clock. }
    \label{fig:ptp-lifecycle}
\end{figure*}

PTP clients traverse multiple stages in a lifecycle to synchronize their clock (Figure~\ref{fig:ptp-lifecycle}). At any point, unexpected conditions such as loss of connectivity can lead PTP to return to an earlier stage in the lifecycle, potentially changing the operating conditions.

\begin{enumerate}[label=S\arabic*.]
    \item \textbf{Discovery} is the stage where remote clocks are identified, usually via periodic multicast announcements. Discovered clients are collected into a PTP domain.
    \item \textbf{Best Master Clock Algorithm (BMCA)} is the predefined algorithm used for each peer to determine whether it should become a master clock or a slave, which can be configured using priorities and clock properties~\cite{bmca-deep-dive}. By the end of this phase, each peer will become either a master clock or a slave. Slaves proceed to connect to, and negotiate with, the master clock.
    \item \textbf{Calibration} is a brief phase where no local clock modifications are made yet to allow the estimate of the offset to increase in precision through multiple synchronization intervals and path delay estimates.
    \item \textbf{Clock Step} Having arrived at a reasonably precise estimate of the offset, an attempt is made to immediately rectify the offset by directly resetting the slave's local time. Since this is a direct violation of invariants I2 and potentially I1 (i.e. this is expected to break applications), it is only done when the offset is large (e.g.~$>1$~second) and many PTP profiles usually allow this to happen only once during the initial synchronization where only using clock-slew would take too long.
    \item \textbf{Clock Slew} is the second phase of convergence where time is made to pass slightly slower or slightly quicker via kernel parameters to fine-tune the clocks position, thus further reducing offset. The rate of change is usually limited via software and configuration parameters, e.g. the Linux kernel limits the rate of change to 500 parts per million, equivalent to 0.05\%~\cite{adjtimex}. These limits ensure that the clock signal does not drift too rapidly, but can make convergence slow -- the 0.05\% limit implies that correcting 1 second of offset takes at least 33 minutes.
    \item \textbf{Stable} is the phase where the clock offset cannot be reduced further, and therefore the clocks are as synchronized as they can get. In this phase, further clock slew is happening to correct clock frequency differences and slight variations still occur due to lack of offset estimation precision, but at this point the clock signal is the most stable and most useful.
\end{enumerate}

For calculating metrics on the quality of the clock synchronization, it is important to consider what should be measured. Often, comparisons are only meaningful when PTP is in comparable states (stable synchronization is usually of most interest), but ensuring this is the case is not always trivial, especially across vendors.
%pre-process the data to exclude data during the convergence phase and only consider observations from the synchronized/stable state. We apply this processing offline by determining the rate of change of the estimated offset's sign -- when frequent switches from positive to negative happen then the algorithm is no longer sure whether its own clock is early or tardy, thus we are close to the optimum synchronization.


\subsection{Synchronization Performance}
It is generally understood that network clock synchronization accuracy is a function of the signal propagation delay and its variance~\cite{managin-pdv-for-ptp}\todo{An evaluation of this using our data would be nice}. A naive approach to clock synchronization that just sends a timestamp from the master to the slave would always be off by the propagation delay, but PTP uses path delay estimation compensate for the propagation delay thus increasing the accuracy. In an ideal world where there is no packet delay variation, the propagation delay could be mitigated entirely, but in reality we have multiple software and hardware components, like the kernel, network stack and network hardware, that each introduce latency variability, which in turn worsens the performance of the delay compensation. Effects, such as asymmetric latencies caused by uneven loads, that cannot easily be compensated for further reduce the synchronization accuracy. Thus, limiting the packet delay variation becomes a primary concern when tuning for precision and dependability.

\subsection{Hardware Support}

\begin{figure}
    \newcommand{\timestampClock}[1][100]{\textcolor{black!#1}{\faClock[regular]}}

    \begin{tikzpicture}[
        start chain=components going below,
        start chain=components2 going below,
        component block/.style={draw, minimum height=0.75cm},
        node distance=0cm,
    ]

            \begin{scope}[name prefix=stack1-, component/.style={component block, text width=3.5cm}]
                \node[on chain=components, component] at (0, 0) (PTP) {PTP Client \hfill \rotatebox{45}{\faStamp{}} \timestampClock{} \faEnvelope[regular]};
                \node[on chain=components, component] (Kernel) {Kernel \hfill \timestampClock[60] \faEnvelope[regular]};
                \node[on chain=components, component] (IP-Stack) {IP Stack \hfill \timestampClock[40] \faEnvelope[regular]};
                \node[on chain=components, component] (Hardware Queue) {Hardware Queue \hfill \timestampClock[30] \faEnvelope[regular]};
                \node[on chain=components, component] (NIC) {NIC \faEthernet \hfill \timestampClock[20] \faEnvelope[regular]};

                \node[above=of PTP] (title) {Software PTP};
            \end{scope}

            \begin{scope}[name prefix=stack2-, component/.style={component block, text width=3.5cm}]
                \node[on chain=components2, component] at (5.5, 0) (PTP) {\faEnvelope[regular] \hfill PTP Client};
                \node[on chain=components2, component] (Kernel) {\faEnvelope[regular] \hfill Kernel};
                \node[on chain=components2, component] (IP-Stack) {\faEnvelope[regular] \hfill IP Stack };
                \node[on chain=components2, component] (Hardware Queue) {\faEnvelope[regular] \hfill Hardware Queue};
                \node[on chain=components2, component] (NIC) {\faEnvelope[regular] \faClock[regular] \rotatebox{-45}{\faStamp{}}  \hfill \faEthernet{} NIC};

                \node[above=of PTP] (title) {Hardware PTP};
            \end{scope}

            \draw[Stealth-Stealth, very thick] (stack1-NIC) -- (stack2-NIC) node[midway, draw, fill=white] (network) {\faNetworkWired};

            \node[draw, dashed, below=0.25cm of network] (network-annotation) {\faClock[regular] $\rightarrow$ \faHistory{}\,\textsuperscript{\textbf{*}}};
            \draw[dotted] (network.south west) -- (network-annotation.north west);
            \draw[dotted] (network.south east) -- (network-annotation.north east);

    \end{tikzpicture}
    \caption{
        Timestamping for PTP messages when using software timestamping (left) and hardware timestamping (right). For software timestamping, the timestamp is generated inside the PTP client and traverses many layers in both egress and ingress, causing additional path delay and delay variation. With hardware timestamping, a timestamp is only added to the message just before it is written to wire by the NIC, thus ensuring a more up-to-date timestamp is sent to the network.\\
        \textsuperscript{\textbf{*}}Network hardware (switches, routers, etc.) also introduces queuing delays. Special PTP-aware hardware can compensate for its own delays to further improve timestamping quality.
    }
    \label{fig:ptp-sw-hw}
\end{figure}

Since delay variation is a primary concern, techniques have been developed to reduce the variability. While PTP can run entirely in software, the path that packets need to traverse between two PTP clients not only includes several hardware components but also some software layers (Figure~\ref{fig:ptp-sw-hw} left). Each component along the path introduces latency and packet delay variation, deteriorating the signal quality. With the appropriate hardware support, message timestamps can instead be generated directly by the NIC driver/hardware (Figure~\ref{fig:ptp-sw-hw} right), which ensures that the timestamp is not affected by the layers above it. This increases the clock synchronization's resilience against interference from adverse conditions, such as high network-, CPU- or kernel load.

However, hardware timestamping alone does not guarantee a high level of dependability. In order to determine what can go wrong, we need to take a look at what constraints the timing system needs to observe and what might lead to potential failures.
