\section{Learnings and Conclusion}
\label{sec:learnings_conclusion}
\label{sec:discussion}
\label{sec:learning}
From our experiments we derive the following learnings and best practices:

\paragraph{Choice of Vendor} We have found there to be substantial synchronization performance and reliability differences between the vendors. While deploying PTPd may be tempting due to its simplicity and maturity, the fact that it can soft-brick the network driver along with its often order of magnitudes slower clock convergence\todo{This should be mentioned explicitly somewhere in the baseline} should be enough of a red flag for projects to steer clear of it (and perhaps its derivatives), even if it can offer comparable accuracy with less effort required for setup (of the tested vendors, it was nevertheless the least accurate). Meta's SPTP was developed specifically with datacenter applications in mind and claims resource consumption advantages, however at the time of writing it comes with some limitations (the reduction in data and packet rates appears to be a tradeoff with higher ROM, RAM and compute usage), and most importantly a lack of maturity due to the recent release (this can be easily confirmed by a quick inspection of the documentation). This leaves LinuxPTP for PTP and Chrony as the NTP alternative -- perhaps surprisingly, Chrony appears to outperform LinuxPTP in terms of synchronization quality on our system. However, Chrony requires several times more ROM space, thus making it more difficult to deploy in resource-constrained environments. In the end, both are quite mature and widely adopted, the better fit ultimately depends on the type of deployment.

\paragraph{Guarding against Resource Contention}
When dealing with resource contention, we observe that network congestion is the primary cause of degradation and that the ideal setup physically separates PTP traffic from application traffic (using e.g. a secondary management network). While certain deployments (e.g. datacenter or industrial IoT) might already feature secondary networks, this solution is often an expensive proposition and embedded deployments might opt for the software solution of traffic prioritization instead. However, setting a DSCP priority is not a cure-all for this case: we have shown that in some of our setups applying prioritization can lead to a considerable decrease in synchronization quality (this anomaly likely depends on the combination of priorities, NIC hardware queues and queuing disciplines, as well as network hardware). Thus, using a software prioritization solution requires proper testing before it can be relied upon. Using the default configuration of no prioritization at all is not recommended, as the 95\textsuperscript{th} percentile clock offset under network load can reach as high as \cmpLoad{net_load_vs_baseline}\fRatio[-2]{\cmpMax} the baseline's 95\textsuperscript{th} percentile. Other resources can also play minor roles in the clock synchronization accuracy (with cache contention and memory bandwidth bottlenecks being the most notable), however they do not get anywhere near the magnitude of network contention.

\paragraph{Resilience against Faults} While the most beneficial piece of hardware support for PTP might seem to be a NIC's hardware timestamping capability to protect against queuing and network delays, it turns out that it is actually more important to have an external clock source available to protect against unexpected conditions when failures occur. We strongly recommend using at least a real-time clock (or a better quality clock source such as GNNS) on the master node (and preferably on slave nodes too) to mitigate inconsistencies that inevitably happen when a node loses its state and therefore its system time. Alternatively, if internet access is available, then the master can import its time from a reliable network source, which achieves the same purpose. In lieu of these solutions (e.g. in isolated embedded settings), careful PTP configuration is necessary, as with the defaults edge cases may occur where clocks differ on the order of magnitude of minutes or years indefinitely, something that cannot be fixed solely by e.g. deploying a failover.

%Lessons learned:
%\begin{itemize}
%    \item Don't use PTP:
%    \begin{itemize}
%        \item Convergence order of magnitude slower
%        \item Faults can bring down entire network interface
%    \end{itemize}
%    \item Prefer physical isolation, prioritization needs to be considered carefully
%    \item Prefer real-time clock (at least on master)
%\end{itemize}

While time synchronization capabilities across packet-switched networks are evolving especially with the advent of TSN, it will probably be some time before we see out-of-the-box synchronization capabilities that are dependable and resilient enough to allow the deployment of synchronization-free distributed algorithms that operate at a timescale of interest to real-time systems while maintaining strict correctness requirements. There are simply too many ways yet to break any bound one might place on a clock signal difference through either bad configuration or hardware limitations. Thus, for the time being, the most reliable way to enforce ordering continues to be manual synchronization as part of a distributed algorithm. However, we invite readers to use the experimental capabilities offered by \toolName{} to evaluate upcoming hardware and software capabilities to determine whether this might change in the future, and we hope that our best practices prove useful for real-world deployments.
\todo{Page limit: 11 pages}
