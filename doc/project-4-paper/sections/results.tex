
\section{Baseline Results}

\subsection{Baseline: The 1-to-1 benchmark}

We start by establishing a baseline for each tested system and vendor which we can later use as a comparison for different environment configurations. We collect data in 20 minute runs (corresponding to just above 1000 samples per run), while retaining the default setting of one synchronization and one measurement of the path delay per second, as our results suggest that a higher frequency does not significantly improve clock synchronization while it does negatively impact the stability of the measurement\todo{This is stated but not shown. Do we need to show?}.

\begin{figure}
    \includegraphics[width=\linewidth]{res/generated/base/rpi08-convergence_2.pdf}
    \caption{A sample run of LinuxPTP in its default configuration. Clock synchronization can follow different convergence trajectories, which needs to be accounted for when calculating statistics. Because PTP uses path delay compensation, the final clock synchronization accuracy is much better than the one-way path delay.}
    \label{fig:baseline_sample}
\end{figure}

Figure \ref{fig:baseline_sample} shows a sample run of the baseline for the PTPd vendor. In order to collect meaningful statistics, we apply some preprocessing to the collected profiles. The most important step is to remove the convergence phase (Figure \ref{fig:baseline_sample} left) from the profile to avoid it skewing the summary statistics. However, the clock travels on a different trajectory during each convergence, and a trajectory may include discontinuities, rebounds, and clock steps. To eliminate this unwanted data, we apply a heuristic to the raw clock offset (which can have positive or negative values depending on the offset direction): If the sign of the offset flips repeatedly over a specified window of time, this implies that there is no clear pattern to the measured offset that PTP can compensate for, and thus the clock offset is stable at its minimum value. Intuitively, this is somewhat equivalent to the average signed clock offset being zero, but note that the two are distinct for certain corner cases, as there are ways to produce an average zero clock offset that still exhibit a signal trend (which we want to avoid) and there can be measurement outliers that cause a non-zero average clock offset while showing no trend. Empirically, we find that using the frequency of sign switches in the clock offset produces much more accurate predictions of whether a clock is synchronized than relying on average (signed) offset.

\cmpSearchVendor{\ptpKey{base/rpi-4/\vendor/pd/q50}/\ptpKey{base/rpi-4/\vendor/q50}}
From Figure \ref{fig:baseline_sample} we observe that the clock offset is much lower than the corresponding path delay, due to the path delay compensation used by PTP clients. Across the tested vendors, we observe that the median absolute clock offset is between \fRatio{\cmpMin} (\fVendor{\cmpMinArg}) and \fRatio{\cmpMax} (\fVendor{\cmpMaxArg}) smaller than the magnitude of the path delay on the Raspberry-Pi 4 cluster,%
\cmpSearchVendor{\ptpKey{base/rpi-5/\vendor/pd/q50}/\ptpKey{base/rpi-5/\vendor/q50}}%
and \fRatio{\cmpMin} (\fVendor{\cmpMinArg}) to \fRatio{\cmpMax} (\fVendor{\cmpMaxArg}) smaller on the Raspberry-Pi 5 cluster, where hardware support is available.
Logically, the absolute clock difference exhibits some amount of skew due to the fact that the value cannot be negative, and the signal can experience bursts of noise which makes determining the true clock offset more difficult. Across the profile however, the median clock offset represents the best estimate of the real clock offset, while the 95\textsuperscript{th} percentile can be used as a bound where one can be reasonably sure that the true offset is lower.

\subsection{Vendors \& Systems}

\begin{figure}
    \includegraphics[width=\linewidth]{res/generated/base/vendor_comparison.pdf}
    \caption{Median baseline performance for all vendors, across our two hardware testbeds (left) and magnified for only the Raspberry-Pi 5 cluster (right). Error bands represent the 5\textsuperscript{th} and 95\textsuperscript{th} percentiles, respectively.}
    \label{fig:baseline}
\end{figure}

\begin{table}
\centering
\caption{Baseline Values by Vendor and System}

    \begin{tikzpicture}[start chain=rows going below, on grid, node distance=0.35cm and 2cm, text width=2cm, align=right]
        \node[on chain=rows] {};
        \begin{scope}[start branch=row going right]
            \node[on chain] {};
            \node[on chain] {Clock Offset};
            \node[on chain] {Clock Offset};
        \end{scope}

        \node[on chain=rows] {System};
        \begin{scope}[start branch=row going right]
            \node[on chain] {Vendor};
            \node[on chain] {Median};
            \node[on chain] {$P_{95}$};
        \end{scope}

            \foreach \system in {rpi-4,rpi-5}{
                \foreach  \vendor  in {ptpd,linuxptp,sptp,chrony}{
                    \node[on chain=rows] {\strcmpfullexpand{\vendor}{ptpd}{\fCluster{\system}}{}};

                    \begin{scope}[start branch=row going right]
                    \node[on chain] {\fVendor{\vendor}};
                    \node[on chain] {\fTimeKey[1]{base/\system/\vendor/q50}};
                    \node[on chain] {\fTimeKey{base/\system/\vendor/q95}};

                    \end{scope}
                }
            }
    \end{tikzpicture}
    \label{tbl:baseline}
\end{table}

\cmpSearchVendor{\ptpKey{base/rpi-5/\vendor/q50}}
\cmpSave{base/rpi-5}
\cmpSearchVendor{\ptpKey{base/rpi-4/\vendor/q50}}
\cmpSave{base/rpi-4}

% Best and worst vendors are the same
\assert{\ptpKey{cmp/base/rpi-4/minarg}}{\ptpKey{cmp/base/rpi-5/minarg}}
\assert{\ptpKey{cmp/base/rpi-4/maxarg}}{\ptpKey{cmp/base/rpi-5/maxarg}}


A logical first step is comparing each vendor's baseline across the systems. Figure~\ref{fig:baseline} shows the four vendors on the Raspberry-Pi 4 and Raspberry-Pi 5 clusters, with the precise values available in Table~\ref{tbl:baseline}. We observe that \fVendor{\cmpMaxArg} has the worst synchronization offset on both systems, with a median clock offset of \fTime[1]{\cmpMax} on the Raspberry-Pi 4 and \fTimeKey[1]{cmp/base/rpi-5/max} on the Raspberry-Pi 5.
\fVendor{\cmpMinArg}, on the other hand, has the best performance on both systems, with the clock offset estimate at just \fTime[1]{\cmpMin} and \fTimeKey[1]{cmp/base/rpi-5/min}, respectively.
\assert{\cmpMinArg}{chrony}%
That a regular NTP client can outperform all of our PTP clients might come as a surprise, with PTP being engineered specifically for precision, but Chrony is very much state-of-the-art and can take advantage of the same hardware acceleration that our PTP clients can while providing a lot more features.

\cmpSearchVendor{\ptpKey{base/rpi-4/\vendor/q50}/\ptpKey{base/rpi-5/\vendor/q50}}

The most noticeable effect on the synchronization quality is the hardware, which is expected since the Raspberry-Pi 5 offers hardware timestamping while the Raspberry-Pi 4 does not. The advantage ranges from \fRatio{\cmpMin} for \fVendor{\cmpMinArg} to \fRatio{\cmpMax} for \fVendor{\cmpMaxArg}. Curiously, although PTPd does not support hardware timestamping, it still offers around \fPercentage{\ptpKey{cmp/base/rpi-5/min}/\ptpKey{base/rpi-5/ptpd/q50}} of the performance of the top contender \fVendor{\ptpKey{cmp/base/rpi-5/minarg}}. While the difference is significant, it definitely falls short of an order of magnitude, showing that hardware timestamps alone are not a cure-all solution to clock-synchronization and cannot eliminate timing variations entirely.

%\todo{
%Advantages: \foreach \vendor in {ptpd,linuxptp,sptp,chrony}{\fRatio{\ptpKey{base/rpi-4/\vendor/q50}/\ptpKey{base/rpi-5/\vendor/q50}} }
%}

\cmpSearchVendor{\ptpKey{base/rpi-4/\vendor/q95}/\ptpKey{base/rpi-4/\vendor/q50}}%
Another aspect to notice is the difference between the median clock offset and the 95\textsuperscript{th} percentile. Without hardware support, this difference can be rather large and has a high spread (between \fRatio{\cmpMin} for the most stable vendor \fVendor{\cmpMinArg} and \fRatio{\cmpMax} for the least stable vendor \fVendor{\cmpMaxArg}),
whereas the magnitude is smaller when hardware support is offered on the Raspberry-Pi %
\cmpSearchVendor{\ptpKey{base/rpi-5/\vendor/q95}/\ptpKey{base/rpi-5/\vendor/q50}}%
(\fRatio[1]{\cmpMin} \fVendor{\cmpMinArg} -- \fRatio[1]{\cmpMax} \fVendor{\cmpMaxArg}).
This means that not only is the average performance improved, but the magnitude of outliers is reduced, which is especially important when considering the dependability aspect. For applications that need to rely on timing sources, this shows that hardware acceleration can make a significant impact, but of course this comes with the price tag associated with a more capable network interface.

\subsection{Reproducibility}
{

\pgfkeys{
    /reproducibility/rpi-4/.cd,
    ptpd/min/.initial={0.000005329},
    ptpd/median/.initial={0.000005976},
    ptpd/max/.initial={0.000024082},
    linuxptp/min/.initial={0.0000041995},
    linuxptp/median/.initial={0.0000049845},
    linuxptp/max/.initial={0.0000059645},
    sptp/min/.initial={0.000002234},
    sptp/median/.initial={0.000002448},
    sptp/max/.initial={0.0000027660000000000003},
    chrony/min/.initial={0.00000125},
    chrony/median/.initial={0.0000012990000000000002},
    chrony/max/.initial={0.0000014060000000000002},
}
\renewcommand{\ptpKeyPrefix}{/reproducibility/rpi-4}


\begin{figure}
    \includegraphics[width=\linewidth]{res/generated/base/key_metric_variance.pdf}
    \caption{Validating the baseline results by repeatedly measuring the baseline for both vendors on the Raspberry Pi 4 system. Top: The median absolute clock offset for each run, with error bars reaching from quantiles 0.05 to 0.95. Bottom: The same for the estimated path delay.}
    \label{fig:baseline_reproducibility}
\end{figure}

\newcommand{\numBaselineMeasurements}{10}
\newcommand{\baselineMinutesRuntime}{\numBaselineMeasurements*4*2*20}

The next question to be answered is how reproducible the measurement results are. Previously, all baseline results were aggregated into a single metric, but in reality the baseline consists of a number of independent measurements, which are measured interleaved for better compensation of noise that may be introduced through mechanisms outside our control. We repeated the measurement of the baseline observations ten times for each vendor on each hardware platform (totaling in around \fpeval{round(\baselineMinutesRuntime/60)} hours of runtime and \fpeval{round(\baselineMinutesRuntime*60)} samples collected) and aggregate them. Between each measurement run, the entire cluster is restarted to ensure that no state is carried over, which would harm the independence of observations. Otherwise, the setup is left untouched, so the only differences occur in the internal state of PTP.

\cmpSearchVendor{\ptpKey{\vendor/max}/\ptpKey{\vendor/min}}

Figure \ref{fig:baseline_reproducibility}\todo{Color scheme messed up on this one.} shows the results for two vendors, PTPd and LinuxPTP. We observe that LinuxPTP produces significantly more stable results for both the clock offset estimation and the path delay, while PTPd shows more variance in median and 95-th quantile observed clock offset, while additionally being less sure about the path delay. A simple restart of the PTP client can suddenly cause the median latency to jump from \fTimeKey{ptpd/min} up to \fTimeKey{ptpd/max}, which corresponds to an increase of \fRelative[-1]{\ptpKey{ptpd/max}/\ptpKey{ptpd/min}} not only momentarily, but throughout an entire run of 20 minutes. This already comes uncomfortably close to our safety factor of \safetyMargin, and we have not even started stressing the system yet. Fortunately, LinuxPTP produces a lot more stable results, with a smaller range of \fTimeKey{linuxptp/min} and \fTimeKey{linuxptp/max} between the best observed run and the worst observed run corresponding to just \fRelative{\ptpKey{linuxptp/max}/\ptpKey{linuxptp/min}} difference. The best vendor in this regard is \fVendor{\cmpMinArg} (not shown), with a range of just \fRelative{\cmpMin}, while the worst performance is observed with the aforementioned \fVendor{\cmpMaxArg}.
%
% Assert last sentence
\assert{\cmpMaxArg}{ptpd}
%
Needless to say, a vendor that cannot deliver stable timing guarantees is risky to deploy in a production environment that needs to rely upon the time source functioning. In the upcoming sections, we will examine whether PTP clients can be made resilient against potential external and internal influences.

}

\section{Resource Contention}

One aspect that influences how reliable PTP can operate is the outside interference originating from resource sharing. Through our research, we identified two key resources which need to be carefully managed so that PTP can provide synchronization even in the presence of stress: the network and the CPU.\todo{Update}

\subsection{Network Contention}
\begin{figure}
    \includegraphics[width=\linewidth]{res/generated/net_unprioritized_trend_rpi-4.pdf}
    \caption{Clock synchronization accuracy at different levels of network load. All vendors experience degradation at higher network loads, though the degree to which they are affected differs. Apart from a change in median clock offset, the 95\textsuperscript{th} percentile is heavily affected, signaling larger and more frequent outliers.}
    \label{fig:network_load}
\end{figure}

\cmpSearchVendor{\ptpKey{load/net_unprioritized/load_100/rpi-4/\vendor/q50}/\ptpKey{base/rpi-4/\vendor/q50}}

Like stated previously, the synchronization accuracy principally depends on the magnitude and the variation of the path delay. Intuition therefore suggests that the more network load is present, the more PTP will struggle to measure the clock offset consistently due to increased queuing in software and hardware causing delay and delay variation in delivery. We simulate network load artificially using iPerf, a traffic generator that allows us to target our predefined data rates. By default, when a PTP vendor is installed on a target system and run, there are no special precautions in place to guard against contention, so a decrease in accuracy as the network load increases is exactly what we observe (Figure \ref{fig:network_load}). Interestingly, the vendor with the smallest increase in clock offset is \fVendor{\cmpMinArg}, with an increase of just \fRatio[1]{\cmpMin} (\fTimeKey{base/rpi-4/\cmpMinArg/q50} -- \fTimeKey{load/net_unprioritized/load_100/rpi-4/\cmpMinArg/q50}) from no network load to 100\% network load. On the other hand, \fVendor{\cmpMaxArg} has a much higher ratio of \fRatio{\cmpMax} (\fTimeKey{base/rpi-4/\cmpMaxArg/q50} -- \fTimeKey{load/net_unprioritized/load_100/rpi-4/\cmpMaxArg/q50}), which can only be partially attributed to the fact that \fVendor{\cmpMaxArg} has a smaller baseline value.
% Assert SPTP is worst
\assert{\cmpMaxArg}{sptp}%
SPTP is the only vendor that relies solely on unicast message exchange (a design choice by Meta), whether this is why SPTP appears to be more susceptible to contention is subject to further investigation.%
%
% 95-th percentiles
\cmpSearchVendor{\ptpKey{load/net_unprioritized/load_100/rpi-4/\vendor/q95}/\ptpKey{base/rpi-4/\vendor/q95}}%
The 95\textsuperscript{th} percentiles reflect the behavior of the outliers, and here we can observe even larger ranges, with \fVendor{\cmpMaxArg} now showing a ratio of \fRatio[-1]{\cmpMax} (\fTimeKey{base/rpi-4/\cmpMaxArg/q95} -- \fTimeKey[-1]{load/net_unprioritized/load_100/rpi-4/\cmpMaxArg/q95}). This is clearly above and beyond even very generous safety margins, so we need to examine how the effect of network load can be mitigated.

 Note that under network load PTP sometimes fails to synchronize at all, when it gets stuck in an infinite loop running into transmission timeouts and thus switching between ``state init'' and ``state faulty''. In this case the timing system breaks entirely, and no synchronization can be established at all. These measurements have been excluded from the shown data, but we encountered them rather frequently at high load levels. Preventing this is paramount, as having no synchronization is potentially a lot worse than having a bad quality signal.

\begin{figure}
    \includegraphics[width=\linewidth]{res/generated/net_isolation_comparison.pdf}
    \begin{center}
    \vspace{-0.5cm}
    \sffamily\scriptsize \textbf{U}: Unprioritized \quad\textbf{P}: Prioritized \quad\textbf{I}: Isolated \quad\textbf{B}: Baseline
    \end{center}
    \caption{Different possibilities of isolation network load, compared to the baseline with no load. Both prioritization and physical isolation can often improve performance over the unprioritized default, however only isolation can match the performance of the baseline without resource contention.}
    \label{fig:net_isolation_comparison}
\end{figure}

Depending on the possibility of dedicating exclusive resources to PTP, two principle solutions are viable: a software/hardware prioritization of traffic to reduce interference of lower priority traffic and providing a dedicated network interface for physical isolation. While the latter might seem like an expensive proposition and could therefore be less suited for embedded solutions, other use cases like industrial settings will often already provide a second management interface separate from the application's network. On our Raspberry-Pis, we emulate this by configuring the routing table so only PTP traffic can use the wired interface, with other traffic being relegated to the wireless interface. Figure~\ref{fig:net_isolation_comparison} shows these possibilities, with the default unisolated case on the left and the baseline on the very right.%
\cmpSearchVendor{\ptpKey{load/net_isolated/load_100/rpi-4/\vendor/q50}/\ptpKey{base/rpi-4/\vendor/q50}}%
%Worst: \fRatio[1]{\cmpMax} @ \cmpMaxArg{} and \fRatio[1]{\cmpMin} @ \cmpMinArg
\assertRange{\cmpMax}{1.0}{1.5}%
\assertRange{\cmpMin}{0.6}{1.0}%
Conforming to expectations, a physical isolation of PTP traffic can entirely mitigate the adverse effects of network load, with results sometimes even outperforming the baseline slightly (\fVendor{\cmpMinArg}: \fRelative{\cmpMin}, \fVendor{\cmpMaxArg}: \fRelative{\cmpMax}).
Theoretically, the only interference that could cross the isolation barrier is latency through cross-talk on the software network stack, which does not appear to be an issue here.
The software-only solution, however, cannot quite achieve the same level of performance on our testbed. Prioritization is achieved through differentiated services (via the Differentiated Services Code Point, DSCP), and requires prioritization support on every networking software and hardware component on the critical path to perform optimally. While our switch and operating system claim to support the technology, this does not appear to be sufficient for complete traffic segregation.
\cmpSearchVendor{\ptpKey{load/net_unprioritized/load_100/rpi-4/\vendor/q50}/\ptpKey{load/net_prioritized/load_100/rpi-4/\vendor/q50}}
Compared to no traffic prioritization, activating DSCP results in performance improvements up to \fRatio{\cmpMax} for \fVendor{\cmpMaxArg}.
However, one anomaly exists, which is \fVendor{\cmpMinArg}, where the median vendor performance actually decreases by \fRatio{1/\cmpMin}, while the 5\textsuperscript{th} and 95\textsuperscript{th} percentiles remain the within 3$\times$ the baseline.
\assertRange{\cmpMin}{0.01}{0.1}%
\assertRange{\ptpKey{load/net_unprioritized/load_100/rpi-4/linuxptp/q95}/\ptpKey{load/net_prioritized/load_100/rpi-4/linuxptp/q95}}{0.3}{1.0}%
\assertRange{\ptpKey{load/net_unprioritized/load_100/rpi-4/linuxptp/q5}/\ptpKey{load/net_prioritized/load_100/rpi-4/linuxptp/q5}}{0.3}{1.0}%
%
\newcommand{\numTrials}{min(\ptpKey{load/net_unprioritized/load_100/rpi-4/linuxptp/count}, \ptpKey{load/net_prioritized/load_100/rpi-4/linuxptp/count})}%
We have examined the affected profiles in more detail and can confirm that this effect is visible on all \fNum{\numTrials} trials encompassing approximately \fNum{\numTrials * 20 * 60} samples, so while we are confident of the validity of the observation, the reason why it occurs is currently unknown.

\todo{Perhaps recommendations}

\subsection{Other shared resources}

\cmpSearchVendor{\ptpKey{load/cpu_unprioritized/load_100/rpi-4/\vendor/q50}/\ptpKey{base/rpi-4/\vendor/q50}}
\cmpSave{q50}
\cmpSearchVendor{\ptpKey{load/cpu_unprioritized/load_100/rpi-4/\vendor/q95}/\ptpKey{base/rpi-4/\vendor/q95}}
\cmpSave{q95}

We have examined the effect of contention on other shared resources, and fortunately none of them cause synchronization quality degradation at the level that network congestion does.
An educated guess for the next most important shared resource could be CPU, as difficulties to get scheduled timely intuitively could cause delays in packet processing.
However, our data shows this is not the case on the Raspberry-Pi 4, as CPU contention only causes a maximum observed median degradation of just \cmpLoad{q50}\fRelative{\cmpMax} (\fVendor{\cmpMaxArg}) and \cmpLoad{q95}\fRelative{\cmpMax} at the 95\textsuperscript{th} percentile on the Raspberry-Pi 4, which is small enough not to make a practical difference.
\assert{\cmpMaxArg}{chrony}
Interestingly, CPU load can actually boost performance relative to the baseline for \cmpLoad{q50}\fVendor{\cmpMinArg} by around \fRelativeInverted{\cmpMin} for the median value and \cmpLoad{q95}\fRelativeInverted{\cmpMin} for the 95\textsuperscript{th} percentile (\ptpKey{load/cpu_unprioritized/load_100/rpi-4/\cmpMinArg/count} trials, $\sim$\fNum{\ptpKey{load/cpu_unprioritized/load_100/rpi-4/\cmpMinArg/count} * 20 * 60} samples).%
\cmpLoad{q50}\assertRange{\cmpMin}{0}{0.8}% Anything below 1 is decrease
\cmpLoad{q95}\assertRange{\cmpMin}{0}{0.8}% Anything below 1 is decrease
\assert{\ptpKey{cmp/q50/minarg}}{\ptpKey{cmp/q95/minarg}}%
%
We assume that this benefit originates from less scheduling variance when the processor is kept busy due to less power saving.
\cmpSearch{\vendor}{chrony,linuxptp}{\ptpKey{load/cpu_unprioritized/load_100/rpi-5/\vendor/q50}/\ptpKey{base/rpi-5/\vendor/q50}}%
\assertRange{\cmpMax}{1.0}{1.2}%
On the Raspberry-Pi 5, things look a little different. While Chrony and LinuxPTP are mostly unaffected (up to just \fRelative{\cmpMax} overhead),
\cmpSearch{\vendor}{sptp,ptpd}{\ptpKey{load/cpu_unprioritized/load_100/rpi-5/\vendor/q50}/\ptpKey{base/rpi-5/\vendor/q50}}%
\assertRange{\cmpMin}{1.6}{3.0}%
SPTP and PTPd have a more noticeable degradation in synchronization performance of up to \fRatio[1]{\cmpMax}, which is however not nearly as critical as the previous degradation through network congestion.

\cmpSearchVendorCluster{\cmpRatioVendorClusterVsBaseline{load/aux_cache/load_100}}%
\cmpSave{cache}%
The third principal hardware component where contention can occur is the memory hierarchy. Generally, PTP does not require a huge amount of resources, and we can see this reflected in the sensitivities against cache contention (between \fRatio[1]{\cmpMin} with \fVendorCluster{\cmpMaxArg} and \fRatio[1]{\cmpMax} with \fVendorCluster{\cmpMaxArg})\todo{Await results},%
\cmpSearchVendorCluster{\cmpRatioVendorClusterVsBaseline{load/aux_memory/load_100}}%
\cmpSave{memory}%
which is slightly higher than what we observe for memory bandwidth contention (up to \fRatio{\cmpMax} with \fVendorCluster{\cmpMaxArg}).
\assertRange{\cmpKey{memory}{max}}{0}{\cmpKey{cache}{max}*0.75}%
In the end, both memory bandwidth contention and cache contention turn out to be more important the CPU contention.%
% q50 is cpu_unprioritized, ugh.
\assertRange{\cmpKey{q50}{max}}{0}{\cmpKey{memory}{max}*0.75}%

We have put the PTP clients through further stress tests (leveraging the capabilities provided by Stress-NG) and observe the following:%
\begin{itemize}
    \item Stressing time-related kernel features such as timers or alarms do not impact PTP performance significantly (with a maximum deviation from the baseline of under 50\%).
    \cmpSearchVendorCluster{\cmpRatioVendorClusterVsBaseline{load/aux_timer/load_100}}%
    \assertRange{\cmpMax}{0.5}{1.5}%
    \cmpSearchVendorCluster{\cmpRatioVendorClusterVsBaseline{load/aux_alarm/load_100}}%
    \assertRange{\cmpMax}{0.5}{1.5}%
    \item Installing cyclic tasks with a deadline in the schedule, which might cause PTP to get deferred when a real-time task needs to be scheduled, also do not cause significant adverse effects. This is good news as applications that require precise notions of time will often also be time-critical, and thus they usually use a real-time scheduling policy. However, note that things will likely start to fall apart when real-time tasks hog almost all available compute, as this will make it difficult for PTP to get scheduled at all unless it is promoted to a corresponding scheduling policy itself.
    \cmpSearchVendorCluster{\cmpRatioVendorClusterVsBaseline{load/aux_cyclic/load_100}}%
    \assertRange{\cmpMax}{0.5}{1.5}%
    \item Finally, all PTP implementations also show good resilience against excessive context switching, which places stress on multiple software and hardware components simultaneously.
    \cmpSearchVendorCluster{\cmpRatioVendorClusterVsBaseline{load/aux_switch/load_100}}%
    \assertRange{\cmpMax}{0.5}{1.5}%
\end{itemize}


%Interestingly, some level of load can actually boost performance relative to the baseline (we observed this on both the CPU and the network).
%We assume this occurs due to the hardware switching less into power saving states when more load is present, which may actually cause packets to be delivered faster.

\section{Fault Tolerance}
\newcommand{\faultLength}{30 seconds}

\begin{figure}
    \begin{center}
    \begin{tikzpicture}[component/.style={draw}, node distance=0.5cm]
        \node[component, ] (master) {\faBug{} \faBolt{} Master};
        \node[component, right=of master] (network) {\faBolt{} Network};
        \node[component, right=of network] (primary-slave) {\faBug{} \faBolt{} Primary Slave};
        \node[component, below=0.25cm of primary-slave] (secondary-slave) {Secondary Slave*};

        \draw[-] (master) -- (network);
        \draw[-] (network) -- (primary-slave);
        \draw[-] (network) |- (secondary-slave);
    \end{tikzpicture}
    \end{center}

    \scriptsize *: For certain outlined scenarios, the secondary slave gets promoted to a failover master when the master fails.

    \caption{
        Simple fault-setup consisting of three clients, where potential software (\faBug{}) or hardware (\faBolt{}) faults may occur. Since we exclude compound failures, only one fault can occur simultaneously, so no faults are triggered in the secondary slave.
    }
    \label{fig:fault_architecture}
\end{figure}

Apart from resource contention, faults are another key threat to the functioning of the timing system. We examine faults originating from three distinct causes: a fault in the network, a fault in the software, and a fault in the hardware. The software/hardware faults can either occur on a PTP slave, in which case they are less critical as the expectation is that the failure will be relatively contained within a single node, or on the PTP master, in which case they are more critical as the failure needs to be contained within the PTP domain. In high-reliability deployments, there will typically be a failover master to take over upon a fault in the master, so we examine this scenario too. Figure~\ref{fig:fault_architecture} shows an overview of the locations and types of faults that may occur. We emulate software faults by hard terminating the PTP client in software -- in reality such a fault might come from within the client due to e.g. a bug or from an external source (e.g. a process kill during an out-of-memory situation). We produce simulated hardware faults using our programmable power delivery unit, which is equivalent to a loss-of-power scenario, but hardware faults and system hang-ups can also come from other component failures.

\subsection{Software Fault -- Slave}

\cmpSearchVendor{\ptpKey{fault/software/slave/rpi-4/\vendor/count}}
\xdef\bSoftwareFaultNumProfiles{\cmpMax}
\newcommand{\windowOfUncertaintyOneMinute}{(60*(0.05/100))}

\begin{figure}
    \includegraphics[width=\linewidth]{res/generated/fault/software/slave_rpi-4_peer_comparison.pdf}
    \caption{Faults induced by a software crash on Raspberry-Pi 4, with a faulty client (shown on the left) and a second client as a control (right). Just after the fault, an increased offset can be observed as PTP tries to resynchronize the clock. Since these offsets occur randomly, we superimpose \fNum{\bSoftwareFaultNumProfiles} trials for each vendor so that the worst case can be seen.}
    \label{fig:software_fault}
\end{figure}

The simplest possible fault is a PTP client crashing, because the only state that is lost is the state within the PTP slave client application (such as the state of the servo disciplining the clock). While the PTP application is down, the clock will continue to drift at the real clock drift rate minus the compensation rate that was last set by the PTP application. When the synchronization before the crash was stable, this observed drift rate is expected to be comparatively low. However, in the worst case when the clock slew is at the maximum rate (as will often be the case in the early stages of synchronization), the software-driven drift can be up to 0.05\%~\cite{adjtimex}, which can cause a software-induced drift of \fTime{0.05/100} per second of downtime, or \fTimeMS{\windowOfUncertaintyOneMinute} per minute. We wish to determine empirically how big the offset is from a software fault when the clock is stable.

\cmpSearchVendor{\ptpKey{fault/software/slave/rpi-4/\vendor/fault/post_max/max}}

From the results (Figure~\ref{fig:software_fault} left), we can determine that the maximum observed offset after a 1-minute software fault among \fNum{\bSoftwareFaultNumProfiles} trials is \fTime{\cmpMax} for \fVendor{\cmpMaxArg}, which is around \fRatio{\cmpMax/\ptpKey{base/rpi-4/\cmpMaxArg/q50}} worse than the median baseline performance. While this drift is definitely noticeable, it is fortunately not even close to the worst-case window of uncertainty of \fTimeMS{\windowOfUncertaintyOneMinute}, representing just \fPercentage[2]{\cmpMax/\windowOfUncertaintyOneMinute} of the worst-case software induced drift. We also observe that it is possible to get lucky: in some trials there is very little observed offset (just \fTime{\cmpMin} with \fVendor{\cmpMinArg}). In any case, all PTP implementations can quickly reconverge on the clock signal within a few seconds of restarting. Due to a quirk in how the PTP protocol works (slaves request synchronization signal leases from the master with a predetermined expiry), the master node will keep sending synchronization messages to the dead peer, which combined with the second lease that is issued when the PTP slave is restarted can help the peer synchronize quicker due to the higher influx of synchronization messages.

In high reliability deployments, replication is frequently applied to increase dependability. However, in order for replication to be useful, we need to ensure that a failure can be contained to the node of origin. In terms of PTP, this means that the fault on one slave should not affect the quality of synchronization of the other slave. To verify that this is the case, we attached a second control node to the same master (Figure~\ref{fig:software_fault} right), which shows us that there is no meaningful degradation of the performance of other nodes.

One important caveat is to be noted for PTPd: when multiple software faults start occurring (not necessarily in close proximity to each other), problems can start propagating. On our test systems, we observe that upon exactly the third software fault, PTPd will cause the network interface to fail. In fact, no amount of manually bringing the network down and back up or reloading the network interface driver can solve this problem, up until the system is restarted by hand. This is especially critical as it implies that not only will the timing system fail, but it will simultaneously bring down any applications that need to communicate over the interface as well. Ironically, in this case rather than support the deployment high-reliability distributed systems, PTPd will actually harm said reliability. We thus advise caution when deploying PTPd or any of its derivatives that may contain the same bug.

\subsection{Hardware Fault -- Slave}
\cmpSearchVendor{\ptpKey{fault/hardware/slave/rpi-4/\vendor/fault/post_max/max}}
\xdef\maxPiFour{\cmpMax}
\cmpSave{hw_slave_rpi-4}

\cmpSearchVendor{\ptpKey{fault/hardware/slave/rpi-5/\vendor/fault/post_max/max}}
\xdef\maxPiFive{\cmpMax}
\cmpSave{hw_slave_rpi-5}


\begin{figure}
    \includegraphics[width=\linewidth]{res/generated/fault/hardware/slave_cluster_comparison.pdf}
    \caption{A hardware fault on the slave of the Raspberry-Pi 4 cluster (left) and the Raspberry-Pi 5 cluster (right). Both slaves need to fully resynchronize after rebooting, but the Raspberry-Pi 5 has an advantage due to its hardware clock.}
    \label{fig:hardware_fault_slave}
\end{figure}

A slightly more difficult scenario is a hardware fault on the slave, as not only will the internal PTP state be lost, but also the kernel state containing the current system time and the clock drift. This implies that PTP will need to fully reconverge, using the standard step-and-slew approach seen in the baseline. Because the Raspberry-Pi 4 (Figure~\ref{fig:hardware_fault_slave} left) does not contain a real-time clock to preserve the system time across reboots, after a reboot a node will have a system time equal the last time that the time was persisted to disk if fake-hwclock~\cite{fake-hwclock-manpage} is active or a default value (e.g. the epoch in 1970) if fake-hwclock is not active. On our system (Raspbian, which bases on Debian), fake-hwclock is configured, so we observe a temporary deviation of just \cmpLoad{hw_slave_rpi-4}\fTimeMin{\cmpMax} (on \fVendor{\cmpMaxArg}). To fix the large offset, PTP needs to step the clock (breaking invariant I2) and then takes some minutes to reconverge on the previous level of accuracy (in the case of PTPd, the other vendors are generally faster). On the other hand, the Raspberry-Pi 5 can maintain the current time using the hardware clock while it is not running. While the RTC generally has lower resolution than the internal oscillator (a common resolution is 1/32768), it generally has good long-term stability\todo{cite}. This advantage shows itself in the observed maximum offset, which is a comparatively tiny \cmpLoad{hw_slave_rpi-5}\fTime{\cmpMax} (\fVendor{\cmpMaxArg}), a full \fNum{ln(\maxPiFour/\maxPiFive)/ln(10)} orders of magnitude smaller that can easily be compensated by clock slew only. A fault on a system without a hardware clock will thus cause an application to encounter large inconsistencies in timing in a place where it might not expect it (right in the middle of a run), while a simple RTC can mitigate this problem entirely. For deployments that are stuck on systems where an RTC is not viable (e.g. due to cost reasons), it is recommended to configure a delay for the application relaunch to allow PTP to resynchronize first.


\subsection{Network Fault}

Another possible root cause for a fault is the network. While a network outage is generally bad news for reliable distributed systems that need to communicate, a total network outage is ironically easier for PTP to handle than a functioning network with high levels of congestion. Since no state is lost,