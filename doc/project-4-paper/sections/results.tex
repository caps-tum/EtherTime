
\section{Baseline Results}
\label{sec:baseline}

\subsection{Testbed -- Hardware and Software}

To conduct our evaluation we employ a total of four hardware testbeds. These include a cluster consisting of three Raspberry Pi 4 attached to an isolated wired Ethernet network via a single Gigabit Ethernet switch and a second cluster consisting of three Raspberry Pi 5 in the same hardware layout (both on Debian 12 and Linux 6.6.20). There are two key differences relevant to timekeeping between the Raspberry-Pi 4 and the Raspberry-Pi 5: The Raspberry-Pi 5 has support for PTP hardware timestamping on the network interface, and it has an integrated real-time clock (RTC) that can be powered by an external battery, both of which the Raspberry-Pi 4 lacks~\cite{raspberry-pi-datasheets}. Furthermore, we utilize a cluster of four Xilinx ZUBoard 1CGs, which feature a combination of ARM Cortex A53 and ARM Cortex R5F cores. These boards were adapted to run Debian with the standard Xilinx Kernels for access to the larger package repository. The base kernel was 5.15, which we patched for R8152/R8153 support for our secondary Ethernet adapters. However, we observed that under Kernel 5.15 hardware timestamping only worked on the transmission path for this board but not on the receiving path, which degraded synchronization accuracy and prevented SPTP from running at all, so we replaced it with a Xilinx 6.6.10 kernel. Finally, we have NVIDIA Jetson TK-1 boards on 32-bit ARMv7, painstakingly updated to Ubuntu 22.04 LTS for software compability but still running the comparatively ancient NVIDIA-customized 3.10.40 kernel. This selection of hardware is representative of a range of embedded systems/edge devices available between 2014 and 2024, both 32-bit and 64-bit and with/without real-time clocks and hardware timestamping across a range of network interface manufacturers. All clusters are controlled by programmable power delivery units so that we can simulate hardware failures in each component individually.

\subsection{Baseline: The 1-to-1 benchmark}

\newcommand{\configIntervalVsBaselineRatioQuantile}{q50}
\newcommand{\configIntervalVsBaselineRatio}[1]{\ptpKey{config/interval/#1/rpi-4/linuxptp/\configIntervalVsBaselineRatioQuantile}/\ptpKey{base/rpi-4/linuxptp/\configIntervalVsBaselineRatioQuantile}}

% good results for slow sample rate, bad ones for faster
\foreach \i in {-1,+0,+1,+2,+3}{
    \assertRange{\configIntervalVsBaselineRatio{\i}}{0.8}{1.2}
}
\foreach \i in {-2,-3,-4,-5,-6}{
    \assertRange{\configIntervalVsBaselineRatio{\i}}{1.2}{10}
}

We start by establishing a baseline for each tested system and vendor which we can later use as a comparison for different environment configurations. We collect data in 20 minute runs (corresponding to just above 1000 samples per run), while retaining the default setting of one synchronization and one measurement of the path delay per second, as our results suggest that a higher frequency does not significantly improve clock synchronization (we observe the best synchronization quality results for LinuxPTP on the Raspberry-Pi 4 at 2 samples/second or slower) while it does negatively impact the stability of the measurement (for the highest rate setting -7, corresponding to \fpeval{1/(2^(-7))} samples/s, we observe a \fRatio{\configIntervalVsBaselineRatio{-7}} worse median accuracy and \renewcommand{\configIntervalVsBaselineRatioQuantile}{q95}\fRatio{\configIntervalVsBaselineRatio{-7}} at the 95\textsuperscript{th} percentile).

\begin{figure}
    \includegraphics[width=\linewidth]{res/generated/base/rpi08-convergence_2.pdf}
    \caption{A sample run of PTPd in its default configuration (left: scattered raw signal and denoised moving average, right: kernel density estimates). Clock synchronization can follow different convergence trajectories, which needs to be accounted for when calculating statistics. Because PTP uses path delay compensation, the final clock synchronization accuracy is much better than the one-way path delay.}
    \label{fig:baseline_sample}
\end{figure}

Figure \ref{fig:baseline_sample} shows a sample run of the baseline for the PTPd vendor. In order to collect meaningful statistics, we apply some preprocessing to the collected profiles. The most important step is to remove the convergence phase (Figure \ref{fig:baseline_sample} left) from the profile to avoid it skewing the summary statistics\todo{slightly duplicated}. However, the clock travels on a different trajectory during each convergence, and each trajectory may include discontinuities, rebounds, and clock steps. To eliminate this unwanted data, we apply a heuristic to the raw clock offset (which can have positive or negative values depending on the offset direction): If the sign of the offset flips repeatedly over a specified window of time, this implies that there is no clear pattern to the measured offset that PTP can compensate for, and thus the clock offset is close to stable at its minimum value. Intuitively, this is somewhat equivalent to the average signed clock offset being zero, but note that the two are distinct for certain corner cases, as there are ways to produce an average zero clock offset that still exhibit a signal trend (which we want to avoid) and there can be large magnitude measurement outliers that cause a non-zero average clock offset while showing no trend. Empirically, we find that using the frequency of sign switches in the clock offset produces much more accurate predictions of whether a clock is synchronized than relying on average (signed) offset.

\cmpSearchVendor{\ptpKey{base/rpi-4/\vendor/pd/q50}/\ptpKey{base/rpi-4/\vendor/q50}}
From Figure \ref{fig:baseline_sample} we observe that the clock offset is much lower than the corresponding path delay, due to the path delay compensation used by PTP clients. Across the tested vendors, we observe that the median absolute clock offset is between \fRatio{\cmpMin} (\fVendor{\cmpMinArg}) and \fRatio{\cmpMax} (\fVendor{\cmpMaxArg}) smaller than the magnitude of the path delay on the Raspberry-Pi 4 cluster,%
\cmpSearchVendor{\ptpKey{base/rpi-5/\vendor/pd/q50}/\ptpKey{base/rpi-5/\vendor/q50}}%
and \fRatio[-1]{\cmpMin} (\fVendor{\cmpMinArg}) to \fRatio[-1]{\cmpMax} (\fVendor{\cmpMaxArg}) smaller on the Raspberry-Pi 5 cluster, where hardware support is available.
Logically, the absolute clock difference exhibits some amount of skew due to the fact that the value cannot be negative, and the signal can experience bursts of noise which makes determining the true clock offset more difficult. Across the profile however, the median clock offset represents the best estimate of the real clock offset, while the 95\textsuperscript{th} percentile can be used as a bound where one can be reasonably sure that the true offset is lower.

\subsection{Vendors \& Systems}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{res/generated/base/vendor_comparison.pdf}
    \legend
    \caption{Median baseline performance for all vendors, across our two hardware testbeds (left) and magnified for only the Raspberry-Pi 5 cluster (right). Error bands represent the 5\textsuperscript{th} and 95\textsuperscript{th} percentiles, respectively.}
    \label{fig:baseline}
\end{figure}

\begin{table}
\centering
\caption{Baseline Values by Vendor and System}

    \begin{tikzpicture}[start chain=rows going below, on grid, node distance=0.35cm and 1.5cm, text width=2cm, align=right]
        \node[on chain=rows] {};
        \begin{scope}[start branch=row going right]
            \node[on chain] {};
            \node[on chain] {Clock Offset};
%            \node[on chain] {Clock Offset};
        \end{scope}

        \node[on chain=rows] {System};
        \begin{scope}[start branch=row going right]
            \node[on chain] {Vendor};
            \node[on chain] {Median};
            \node[on chain] {$P_{95}$};
            \node[on chain] {Max};
        \end{scope}

            \foreach \system in {rpi-4,rpi-5,petalinux,tk-1}{
                \foreach  \vendor  in {ptpd,linuxptp,sptp,chrony}{
                    \node[on chain=rows] {\strcmpfullexpand{\vendor}{ptpd}{\fCluster{\system}}{}};

                    \begin{scope}[start branch=row going right]
                    \renewcommand{\ptpKeyUndefinedValue}{-0.000001}
                    \node[on chain] {\fVendor{\vendor}};
                    \node[on chain] {\fTimeKeyOmitIfUndefined[1]{base/\system/\vendor/q50}};
                    \node[on chain] {\fTimeKeyOmitIfUndefined[1]{base/\system/\vendor/q95}};
                    \node[on chain] {\fTimeKeyOmitIfUndefined[0]{base/\system/\vendor/q100}};
                    \end{scope}
                }
            }
    \end{tikzpicture}
    \label{tbl:baseline}
\end{table}

\cmpSearchVendor{\ptpKey{base/rpi-4/\vendor/q50}}
\cmpSave{base/rpi-4}
\cmpSearchVendor{\ptpKey{base/rpi-5/\vendor/q50}}
\cmpSave{base/rpi-5}

\assertMultiCmpMinArgMaxArgSame{base/rpi-4}{base/rpi-5}

\cmpSearchVendor{\ptpKey{base/petalinux/\vendor/q50}}
\cmpSave{base/petalinux}
\assert{\cmpMinArg}{sptp}
\assert{\cmpMaxArg}{ptpd}

\cmpSearchVendorNoSPTP{\ptpKey{base/tk-1/\vendor/q50}}
\cmpSave{base/tk-1}
\assert{\cmpMinArg}{linuxptp}
\assert{\cmpMaxArg}{ptpd}

% Best and worst vendors are the same
%\assertMultiCmpMinArgMaxArgSame{base/rpi-4}{base/petalinux}
%\assertMultiCmpMinArgMaxArgSame{base/rpi-4}{base/tk-1}

\cmpSearch{\cluster}{rpi-4,rpi-5,petalinux,tk-1}{\cmpKey{base/\cluster}{max}/\cmpKey{base/\cluster}{min}}
\cmpSave{base/minmax_difference_percentage}
%\newcommand{\baselineWorseFactor}{\fRelative{min(
%    \cmpKey{base/rpi-4}{max}/\cmpKey{base/rpi-4}{min},
%    \cmpKey{base/rpi-5}{max}/\cmpKey{base/rpi-5}{min},
%    \cmpKey{base/petalinux}{max}/\cmpKey{base/petalinux}{min},
%    \cmpKey{base/tk-1}{max}/\cmpKey{base/tk-1}{min},
%)}}

A logical first step is comparing each vendor's baseline across the systems. Figure~\ref{fig:baseline} shows the four vendors on all four clusters, with the precise values available in Table~\ref{tbl:baseline}. Unfortunately, we had to exclude SPTP on the TK-1 cluster as it was not able to run even with our 32-bit patches, since the 3.10 kernel is missing support for some socket options required by SPTP. We observe that \fVendor{\cmpKey{base/rpi-4}{maxarg}} has the worst synchronization offset on all systems, with a median clock offset between \fRelative{\cmpKey{base/minmax_difference_percentage}{min}} worse than the best performing vendor on \fCluster{\cmpKey{base/minmax_difference_percentage}{minarg}} and \fRelative{\cmpKey{base/minmax_difference_percentage}{max}} on \fCluster{\cmpKey{base/minmax_difference_percentage}{maxarg}}.
\fVendor{\cmpKey{base/rpi-4}{minarg}}, on the other hand, has the best performance on the Raspberry-Pi systems, with the clock offset estimates at \fRelativeInverted{\cmpKey{base/rpi-4}{min}/\cmpKey{base/rpi-4}{mean}} and \fRelativeInverted{\cmpKey{base/rpi-5}{min}/\cmpKey{base/rpi-5}{mean}} lower than the mean performance, respectively. On the Petalinux and TK-1 systems, \todo{INSERT NEW BASELINE DATA ONCE AVAILABLE.}

\assert{\cmpKey{base/rpi-4}{minarg}}{chrony}%
That a regular NTP client can match or outperform all the tested PTP clients might come as a surprise, with PTP being engineered specifically for precision, but Chrony is very much state-of-the-art and can take advantage of the same hardware acceleration that our PTP clients can while providing a lot more features\todo{this claim might need to be backed up}.

\cmpSearchVendor{\ptpKey{base/rpi-4/\vendor/q50}/\ptpKey{base/rpi-5/\vendor/q50}}

The most noticeable effect on the synchronization quality is the choice of hardware, which is expected since the Raspberry-Pi 5 offers hardware timestamping while the Raspberry-Pi 4 does not. The newer hardware's performance advantage ranges from \fRatio{\cmpMin} for \fVendor{\cmpMinArg} to \fRatio{\cmpMax} for \fVendor{\cmpMaxArg}. Curiously, although PTPd does not support hardware timestamping and thus cannot use hardware capabilities, it still offers around \fPercentage{\ptpKey{cmp/base/rpi-5/min}/\ptpKey{base/rpi-5/ptpd/q50}} of the performance of the top contender \fVendor{\ptpKey{cmp/base/rpi-5/minarg}}. While the difference is significant, it is not orders of magnitude better than the software-only implementation, showing that hardware timestamps alone are not a cure-all solution to clock-synchronization and cannot eliminate timing variations entirely.

%\todo{
%Advantages: \foreach \vendor in {ptpd,linuxptp,sptp,chrony}{\fRatio{\ptpKey{base/rpi-4/\vendor/q50}/\ptpKey{base/rpi-5/\vendor/q50}} }
%}

\cmpSearchVendor{\ptpKey{base/rpi-4/\vendor/q95}/\ptpKey{base/rpi-4/\vendor/q50}}%
Another aspect to notice is the difference between the median clock offset and the 95\textsuperscript{th} percentile. Without hardware support, this difference can be rather large and has a high spread (between \fRatio{\cmpMin} for the most stable vendor \fVendor{\cmpMinArg} and \fRatio{\cmpMax} for the least stable vendor \fVendor{\cmpMaxArg}),
whereas the magnitude is smaller when hardware support is offered on the Raspberry-Pi %
\cmpSearchVendor{\ptpKey{base/rpi-5/\vendor/q95}/\ptpKey{base/rpi-5/\vendor/q50}}%
(\fRatio[1]{\cmpMin} \fVendor{\cmpMinArg} -- \fRatio[1]{\cmpMax} \fVendor{\cmpMaxArg}).
This means that not only is the average performance improved, but the magnitude of outliers is reduced, which is especially important when considering the dependability aspect. For applications that need to rely on timing sources, this shows that hardware acceleration can make a significant impact, but of course this comes with the price tag associated with a more capable network interface.

\subsection{Reproducibility}

% Ordering: Requires ptpKeyPrefix to be intact
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{res/generated/base/key_metric_variance_rpi-4.pdf}
    \includegraphics[width=\linewidth]{res/generated/base/ptpd-good-vs-bad.pdf}

    \legend
    \caption{Validating the baseline results by repeatedly measuring the baseline for both vendors on the Raspberry Pi 4 system. Top: The median absolute clock offset for each run, with error bars reaching from quantiles 0.05 to 0.95. Bottom: Timeseries profiles for the best run and the worst run under identical conditions, showing significant differences in noise levels.}
    \label{fig:baseline_reproducibility}
\end{figure}


{
\pgfkeys{
    /reproducibility/rpi-4/.cd,
    ptpd/min/.initial={0.000005329},
    ptpd/median/.initial={0.000005976},
    ptpd/max/.initial={0.000024082},
    linuxptp/min/.initial={0.0000041995},
    linuxptp/median/.initial={0.0000049845},
    linuxptp/max/.initial={0.0000059645},
    sptp/min/.initial={0.000002234},
    sptp/median/.initial={0.000002448},
    sptp/max/.initial={0.0000027660000000000003},
    chrony/min/.initial={0.00000125},
    chrony/median/.initial={0.0000012990000000000002},
    chrony/max/.initial={0.0000014060000000000002},
}
\renewcommand{\ptpKeyPrefix}{/reproducibility/rpi-4}



\newcommand{\numBaselineMeasurements}{10}
\newcommand{\baselineTotalMinutesRuntime}{\numBaselineMeasurements*4*2*20}
\newcommand{\numSamplesPerRunApprox}{\fpeval{round(20*60)}}

A key challenge with conducting high-quality performance studies is figuring out how to aggregate data to increase information density while avoiding the aggregation hiding important events such as outliers. While comparing clock-synchronization quantiles between vendors is certainly useful, in reality this does not tell the entire story because each baseline consists of a number of independent measurements, which in turn consists of around \numSamplesPerRunApprox{} samples, all of which are affected by multiple sources of noise. As it turns out (Figure~\ref{fig:baseline_reproducibility}), not only is there a significant amount of noise during each run\todo{Find a good metric to use here}, but there are also discrepancies between measurement runs. This leads to completely different results for identical measurement runs despite decent measurement lengths and number of samples.
\cmpSearchVendor{\ptpKey{\vendor/max}/\ptpKey{\vendor/min}}

\assert{\cmpMaxArg}{ptpd}
We observe that in absolute terms, PTPd is the main source of variance for both median/95-th percentile observed clock offsets and path delays across independent measurements. A simple restart of the PTPd client can suddenly cause the median latency to jump from \fTimeKey{ptpd/min} up to \fTimeKey{ptpd/max}, which corresponds to an increase of \fRelative[-1]{\ptpKey{ptpd/max}/\ptpKey{ptpd/min}} not only momentarily, but throughout an entire run of 20 minutes.
%This already comes uncomfortably close to our safety factor of \safetyMargin, and we have not even started stressing the system yet.
Fortunately, LinuxPTP produces a lot more stable results, with a smaller range of \fTimeKey{linuxptp/min} and \fTimeKey{linuxptp/max} between the best observed run and the worst observed run, corresponding to just \fRelative{\ptpKey{linuxptp/max}/\ptpKey{linuxptp/min}} difference. The best vendor in this regard is \fVendor{\cmpMinArg}, with both a relative range of just \fRelative{\cmpMin} and an absolute range of \fTime[1]{\ptpKey{chrony/max}-\ptpKey{chrony/min}}.
%
% Assert last sentence
\assert{\cmpMaxArg}{ptpd}
%

To deal with the amount of noise, we apply a variety of noise reduction techniques. All measurements are interleaved for better compensation of noise that may be introduced through mechanisms outside our control, we repeat each observation at least \numBaselineMeasurements{} times for each vendor on each hardware platform (totaling in around \fpeval{round(\baselineTotalMinutesRuntime/60)} hours of runtime and \fpeval{round(\baselineTotalMinutesRuntime*60)} samples collected) and between each measurement run, the entire cluster is restarted to ensure that no state is carried over, which would harm the independence of observations. We further filter data that contains too many holes, undercuts a threshold of the actual number of samples collected, or fails basic consistency checks (all of which are likely to lead to bad quality data). Otherwise, the setup is left untouched, so that the only differences that occur are in the internal state of PTP.


Needless to say, a vendor that cannot deliver stable timing guarantees is risky to deploy in a production environment that needs to rely upon the time source functioning. In the upcoming sections, we will examine whether PTP clients can be made resilient against potential external and internal influences.

}

\section{Resource Contention}
\label{sec:resource_contention}

One aspect that influences how reliable PTP can operate is the outside interference originating from resource sharing. Through our research, we examined a range of shared resources that might impact synchronization accuracy and therefore need to be carefully managed so that PTP can provide synchronization even in the presence of stress. Unsurprisingly, the key resource tends to be the network.

\subsection{Network Contention}
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{res/generated/net_unprioritized_trend_rpi-4.pdf}
    \legend
    \caption{Clock synchronization accuracy at different levels of network load. All vendors experience degradation at higher network loads, though the degree to which they are affected differs. Apart from a change in median clock offset, the 95\textsuperscript{th} percentile is heavily affected, signaling larger and more frequent outliers.}
    \label{fig:network_load}
\end{figure}

\cmpSearchVendor{\ptpKey{load/net_unprioritized/load_100/rpi-4/\vendor/q50}/\ptpKey{base/rpi-4/\vendor/q50}}

Like stated previously, the synchronization accuracy principally depends on the magnitude and the variation of the path delay. Intuition therefore suggests that the more network load is present, the more PTP will struggle to measure the clock offset consistently due to increased queuing in software and hardware causing delay and delay variation in delivery. We simulate network load artificially using iPerf, a traffic generator that allows us to target predefined data rates. By default, when a PTP vendor is installed on a target system and run, there are no special precautions in place to guard against contention, so a decrease in accuracy as the network load increases is exactly what we observe (Figure \ref{fig:network_load}). Interestingly, the vendor with the smallest increase in clock offset is \fVendor{\cmpMinArg}, with an increase of just \fRatio[1]{\cmpMin} (\fTimeKey{base/rpi-4/\cmpMinArg/q50} -- \fTimeKey{load/net_unprioritized/load_100/rpi-4/\cmpMinArg/q50}) from no network load to 100\% network load. On the other hand, \fVendor{\cmpMaxArg} has a much higher ratio of \fRatio{\cmpMax} (\fTimeKey{base/rpi-4/\cmpMaxArg/q50} -- \fTimeKey{load/net_unprioritized/load_100/rpi-4/\cmpMaxArg/q50}), which can only be partially attributed to the fact that \fVendor{\cmpMaxArg} has a smaller baseline value.
% Assert SPTP is worst
\assert{\cmpMaxArg}{sptp}%
SPTP is the only vendor that relies solely on unicast message exchange (a design choice by Meta), whether this is why SPTP appears to be more susceptible to contention is subject to further investigation.%
%
% 95-th percentiles
\cmpSearchVendor{\ptpKey{load/net_unprioritized/load_100/rpi-4/\vendor/q95}/\ptpKey{base/rpi-4/\vendor/q95}}%
\cmpSave{net_load_vs_baseline} % Used in conclusion
The 95\textsuperscript{th} percentiles reflect the behavior of the outliers, and here we can observe even larger ranges, with \fVendor{\cmpMaxArg} now showing a ratio of \fRatio[-1]{\cmpMax} (\fTimeKey{base/rpi-4/\cmpMaxArg/q95} -- \fTimeKey[-1]{load/net_unprioritized/load_100/rpi-4/\cmpMaxArg/q95}). This is clearly above and beyond even very generous safety margins, so we need to examine how the effect of network load can be mitigated.

\begin{figure}
    \includegraphics[width=\linewidth]{res/generated/base/clock_diff_by_path_delay.pdf}
    \caption{Clock differences for all PTP implementations under various tested circumstances and across all systems depending on how high the path delay is and how frequently it varies, with each example representing a 20-minute run.}
    \label{fig:path-delay-all}
\end{figure}

To show that the path delay is a good predictor of the synchronization quality, we can aggregate all 20-minute profiles across all our measurements and plot them by path delay and path delay variation (Figure~\ref{fig:path-delay-all}). This shows us that generally, a high path delay and path delay variation is strongly correlated with a bad synchronization quality. However, each metric alone is not a good indicator as examples of high-quality synchronization are present wither either only high path delay or only high path delay variation. Notice also that there are multiple isolated clusters very far away from the norm (sometimes more than 10$\times$), which, while small, still have multiple examples often originating from the same vendor. This suggests that the internal path delay compensation mechanisms of these vendors can be tripped off balance more or less consistently.

\newcommand{\loadFaultyNumFailures}{9}
\newcommand{\loadFaultyNumTrials}{34}
Note that under heavier network load PTP sometimes fails to even synchronize at all, when it gets stuck in an infinite loop of transmission timeouts indefinitely cycling between ``state init'' and ``state faulty''. In this case the timing system breaks entirely, and no synchronization can be established. These measurements have been excluded from the shown data, but we encountered them rather frequently at high load levels ($\sim$\fPercentage{\loadFaultyNumFailures/\loadFaultyNumTrials} of 20 minute runs for 100\% network load on the Raspberry-Pi 5 did not synchronize at all, with LinuxPTP accounting for most failures while Chrony produced no errors). Preventing this is paramount, as having no synchronization and potentially not even being aware of it is a lot worse than having a bad quality signal.

\begin{figure}
    \includegraphics[width=\linewidth]{res/generated/net_isolation_comparison.pdf}
    \begin{center}
    \vspace{-0.5cm}
    \sffamily\scriptsize \textbf{U}: Unprioritized \quad\textbf{P}: Prioritized \quad\textbf{I}: Isolated \quad\textbf{B}: Baseline
    \end{center}
    \begin{center}
    \legend
    \end{center}
    \caption{Different possibilities of isolating the network load, compared to the baseline with no load. Both prioritization and physical isolation can often improve performance over the unprioritized default, however only isolation can match the performance of the baseline without resource contention.}
    \label{fig:net_isolation_comparison}
\end{figure}

Depending on the possibility of dedicating exclusive resources to PTP, two principle solutions are viable: a software/hardware prioritization of traffic to reduce interference of lower priority traffic and providing a dedicated network interface for physical isolation. While the latter might seem like an expensive proposition and could therefore be less suited for embedded solutions, other use cases like industrial or datacenter settings will often already provide a second management interface separate from the application's network. On our Raspberry-Pis, we emulate this by configuring the routing table so only PTP traffic can use the wired interface, with other traffic being relegated to the wireless interface. Figure~\ref{fig:net_isolation_comparison} shows these possibilities, with the default unisolated case on the left and the baseline on the very right.%
\cmpSearchVendor{\ptpKey{load/net_isolated/load_100/rpi-4/\vendor/q50}/\ptpKey{base/rpi-4/\vendor/q50}}%
%Worst: \fRatio[1]{\cmpMax} @ \cmpMaxArg{} and \fRatio[1]{\cmpMin} @ \cmpMinArg
\assertRange{\cmpMax}{1.0}{1.5}%
\assertRange{\cmpMin}{0.6}{1.0}%
Conforming to expectations, a physical isolation of PTP traffic can entirely mitigate the adverse effects of network load, with results sometimes even outperforming the baseline slightly (\fVendor{\cmpMinArg}: \fRelative{\cmpMin}, \fVendor{\cmpMaxArg}: \fRelative{\cmpMax}).
Theoretically, the only interference that could traverse the isolation barrier is latency caused by cross-talk on the software network stack, which does not appear to be an issue here.

The prioritization-based solution, however, cannot quite achieve the same level of performance on our testbed. Prioritization is achieved through differentiated services (via the Differentiated Services Code Point, DSCP), and requires prioritization support on every networking software and hardware component on the critical path to perform optimally. While our switch and operating system claim to support the technology, this does not appear to be sufficient for complete traffic segregation.
\cmpSearchVendor{\ptpKey{load/net_unprioritized/load_100/rpi-4/\vendor/q50}/\ptpKey{load/net_prioritized/load_100/rpi-4/\vendor/q50}}
Compared to no traffic prioritization, activating DSCP results in performance improvements up to \fRatio{\cmpMax} for \fVendor{\cmpMaxArg}.
However, one anomaly exists, which is \fVendor{\cmpMinArg}, where the median vendor performance actually decreases by \fRatio{1/\cmpMin}, while the 5\textsuperscript{th} and 95\textsuperscript{th} percentiles remain the within 3$\times$ the baseline.
\assertRange{\cmpMin}{0.01}{0.1}%
\assertRange{\ptpKey{load/net_unprioritized/load_100/rpi-4/linuxptp/q95}/\ptpKey{load/net_prioritized/load_100/rpi-4/linuxptp/q95}}{0.3}{1.0}%
\assertRange{\ptpKey{load/net_unprioritized/load_100/rpi-4/linuxptp/q5}/\ptpKey{load/net_prioritized/load_100/rpi-4/linuxptp/q5}}{0.3}{1.0}%
%
\newcommand{\numTrials}{min(\ptpKey{load/net_unprioritized/load_100/rpi-4/linuxptp/count}, \ptpKey{load/net_prioritized/load_100/rpi-4/linuxptp/count})}%
We have examined the affected profiles in more detail and can confirm that this effect is visible on all \fNum{\numTrials} trials encompassing approximately \fNum{\numTrials * 20 * 60} samples, so while we are confident of the validity of the observation, the reason why it occurs is currently unknown.

\todo{Perhaps recommendations}

\subsection{Other shared resources}

\cmpSearchVendor{\ptpKey{load/cpu_unprioritized/load_100/rpi-4/\vendor/q50}/\ptpKey{base/rpi-4/\vendor/q50}}
\cmpSave{q50}
\cmpSearchVendor{\ptpKey{load/cpu_unprioritized/load_100/rpi-4/\vendor/q95}/\ptpKey{base/rpi-4/\vendor/q95}}
\cmpSave{q95}

We have  further examined the effect of contention on other shared resources, and fortunately none of them cause synchronization quality degradation at the level that network congestion does.
An educated guess for the next most important shared resource could be CPU, as difficulties to get scheduled timely intuitively could cause delays in packet processing.
However, our data shows this is not the case on the Raspberry-Pi 4, as CPU contention only causes a maximum observed median degradation of just \cmpLoad{q50}\fRelative{\cmpMax} (\fVendor{\cmpMaxArg}) and \cmpLoad{q95}\fRelative{\cmpMax} at the 95\textsuperscript{th} percentile on the Raspberry-Pi 4, which is small enough not to make a practical difference.
\assert{\cmpMaxArg}{chrony}
This is likely due to the fact that PTP clients do not consume much processing time, which means they can easily get scheduled on Linux's Completely Fair Scheduler (CFS) even when idle computing time is scarce.
Interestingly, CPU load can actually boost performance relative to the baseline for \cmpLoad{q50}\fVendor{\cmpMinArg} by around \fRelativeInverted{\cmpMin} for the median value and \cmpLoad{q95}\fRelativeInverted{\cmpMin} for the 95\textsuperscript{th} percentile (\ptpKey{load/cpu_unprioritized/load_100/rpi-4/\cmpMinArg/count} trials, $\sim$\fNum{\ptpKey{load/cpu_unprioritized/load_100/rpi-4/\cmpMinArg/count} * 20 * 60} samples).
\cmpLoad{q50}\assertRange{\cmpMin}{0}{0.8}% Anything below 1 is decrease
\cmpLoad{q95}\assertRange{\cmpMin}{0}{0.8}% Anything below 1 is decrease
\assert{\ptpKey{cmp/q50/minarg}}{\ptpKey{cmp/q95/minarg}}%
%
We assume that this benefit originates from less scheduling variance when the processor is kept busy due to less power saving.
\cmpSearch{\vendor}{chrony,linuxptp,sptp}{\ptpKey{load/cpu_unprioritized/load_100/rpi-5/\vendor/q50}/\ptpKey{base/rpi-5/\vendor/q50}}%
\assertRange{\cmpMax}{1.0}{1.2}%
On the Raspberry-Pi 5, things look a little different. While Chrony, LinuxPTP and SPTP are mostly unaffected (up to just \fRelative{\cmpMax} overhead),
\cmpSearch{\vendor}{ptpd}{\ptpKey{load/cpu_unprioritized/load_100/rpi-5/\vendor/q50}/\ptpKey{base/rpi-5/\vendor/q50}}%
\assertRange{\cmpMin}{1.6}{3.0}%
PTPd has a more noticeable degradation in synchronization performance of up to \fRelative{\cmpMax}, which is however not nearly as critical as the previous degradation through network congestion.

\cmpSearchVendorCluster{\cmpRatioVendorClusterVsBaseline{load/aux_cache/load_100}}%
\todo{TODO: RE-ENABLE ASSERTION WHEN DATA AVAILABLE}
%\assertRange{\cmpMin}{2.0}{4.0}
\assertRange{\cmpMax}{4.0}{10.0}
\cmpSave{cache}%
The third principal hardware component where contention can occur is the memory hierarchy. Generally, PTP does not require a large amount of memory, but we do observe a moderate amount of sensitivity against cache contention (between \fRatio[1]{\cmpMin} with \fVendorCluster{\cmpMinArg} and \fRatio[1]{\cmpMax} with \fVendorCluster{\cmpMaxArg}),%
\cmpSearchVendorCluster{\cmpRatioVendorClusterVsBaseline{load/aux_memory/load_100}}%
\cmpSave{memory}%
and memory bandwidth contention (up to \fRatio{\cmpMax} with \fVendorCluster{\cmpMaxArg}), though cache contention appears to be slightly more important.
\assertRange{\cmpKey{memory}{max}}{0}{\cmpKey{cache}{max}*0.75}%
In the end, both memory bandwidth contention and cache contention turn out to be more important than CPU contention.%
% q50 is cpu_unprioritized, ugh.
\assertRange{\cmpKey{q50}{max}}{0}{\cmpKey{memory}{max}*0.75}%

We have put the PTP clients through further stress tests (leveraging the capabilities provided by Stress-NG) and observe the following:%
\begin{itemize}
    \item Stressing time-related kernel features such as timers or alarms do not impact PTP performance significantly (with a maximum deviation from the baseline of under 50\%).
    \cmpSearchVendorCluster{\cmpRatioVendorClusterVsBaseline{load/aux_timer/load_100}}%
    \assertRange{\cmpMax}{0.5}{1.5}%
    \cmpSearchVendorCluster{\cmpRatioVendorClusterVsBaseline{load/aux_alarm/load_100}}%
    \assertRange{\cmpMax}{0.5}{1.5}%
    \item Installing cyclic tasks with a deadline in the schedule (\texttt{SCHED\_DEADLINE}), which might cause PTP to get deferred when a real-time task needs to be scheduled, also do not cause significant adverse effects. This is good news as applications that require precise notions of time will often also be time-critical, and thus they usually use a real-time scheduling policy. However, note that things will likely start to fall apart when real-time tasks hog almost all available compute, as this will make it difficult for PTP to get scheduled at all unless it is promoted to a corresponding scheduling policy itself (which is not the case for the default profile).
    \cmpSearchVendorCluster{\cmpRatioVendorClusterVsBaseline{load/aux_cyclic/load_100}}%
    \assertRange{\cmpMax}{0.5}{1.5}%
    \item Finally, all PTP implementations also show good resilience against excessive context switching, which places stress on multiple software and hardware components simultaneously.
    \cmpSearchVendorCluster{\cmpRatioVendorClusterVsBaseline{load/aux_switch/load_100}}%
    \assertRange{\cmpMax}{0.5}{1.5}%
\end{itemize}

While resource contention, especially network congestion, can lead to degradation of clock synchronization performance, it is not the sole reason that the time synchronization system may fail.

%Interestingly, some level of load can actually boost performance relative to the baseline (we observed this on both the CPU and the network).
%We assume this occurs due to the hardware switching less into power saving states when more load is present, which may actually cause packets to be delivered faster.

\section{Fault Tolerance}
\label{sec:fault_tolerance}
\newcommand{\faultLength}{30 seconds}
\todo{Review this: The information density of this section needs to be increased}

\begin{figure}
    \begin{center}
    \begin{tikzpicture}[component/.style={draw}, node distance=0.5cm]
        \node[component, ] (master) {\faBug{} \faBolt{} Master};
        \node[component, right=of master] (network) {\faBolt{} Network};
        \node[component, right=of network] (primary-slave) {\faBug{} \faBolt{} Primary Slave};
        \node[component, below=0.25cm of primary-slave] (secondary-slave) {Secondary Slave*};

        \draw[-] (master) -- (network);
        \draw[-] (network) -- (primary-slave);
        \draw[-] (network) |- (secondary-slave);
    \end{tikzpicture}
    \end{center}

    \scriptsize *: For certain outlined scenarios, the secondary slave gets promoted to a failover master when the master fails.

    \caption{
        Simple fault-setup consisting of three clients, where potential software (\faBug{}) or hardware (\faBolt{}) faults may occur. Since we exclude compound failures, only one fault can occur simultaneously, so no faults are triggered in the secondary slave.
    }
    \label{fig:fault_architecture}
\end{figure}

Apart from resource contention, faults are another key threat to the functioning of the timing system. We examine faults originating from three distinct causes: a fault in the network, a fault in the software, and a fault in the hardware. The software/hardware faults can either occur on a PTP slave, in which case they are less critical as the expectation is that the failure will be relatively contained within a single node, or on the PTP master, in which case they are more critical as the failure could propagate across the PTP domain. In high-reliability deployments, there will typically be a failover master to take over upon a fault in the master, so we examine that scenario too. Figure~\ref{fig:fault_architecture} shows an overview of the locations and types of faults that may occur. We emulate software faults by hard terminating the PTP client in software -- in reality such a fault might come from within the client due to e.g. a bug, or from an external source (e.g. a process kill during an out-of-memory situation). We produce simulated hardware faults using our programmable power delivery unit, which is equivalent to a loss-of-power scenario, but real-life production hardware faults and system hang-ups could also come from other component failures.

\subsection{Software Fault -- Slave}

\cmpSearchVendor{\ptpKey{fault/software/slave/rpi-4/\vendor/count}}
\xdef\bSoftwareFaultNumProfiles{\cmpMax}
\newcommand{\maxClockSlew}{(0.05/100)}
\newcommand{\windowOfUncertaintyOneMinute}{(60*(\maxClockSlew))}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{res/generated/fault/software/slave_rpi-4_peer_comparison.pdf}
    \legend
    \caption{Faults induced by a software crash on Raspberry-Pi 4, with a faulty client (shown on the left) and a second client as a control (right). Just after the fault, an increased offset can be observed as PTP tries to resynchronize the clock. Since these offsets occur randomly, we superimpose \fNum{\bSoftwareFaultNumProfiles} trials for each vendor so that the worst case can be seen.}
    \label{fig:software_fault}
\end{figure}

The simplest possible fault is a PTP client crashing, because the only state that is lost is the state within the PTP slave client application (such as the state of the servo disciplining the clock). While the PTP application is down, the clock will continue to drift at the real clock drift rate minus the compensation rate that was last set by the PTP application. If the synchronization before the crash was stable, this observed drift rate is expected to be comparatively low. However, in the worst case when the clock slew is at the maximum rate (as will often be the case in the early stages of convergence), the software-driven drift can be up to 0.05\%~\cite{adjtimex}, which can cause a software-induced drift of \fTime{0.05/100} per second of downtime, or \fTimeMS{\windowOfUncertaintyOneMinute} per minute. We wish to determine empirically how big the offset is from a software fault when the clock is stable.

\cmpSearchVendor{\ptpKey{fault/software/slave/rpi-4/\vendor/fault/post_max/max}}

From the results (Figure~\ref{fig:software_fault} left), we can determine that the maximum observed offset after a 1-minute software fault among \fNum{\bSoftwareFaultNumProfiles}\todo{import more trials} trials is \fTime{\cmpMax} for \fVendor{\cmpMaxArg}, which is around \fRatio{\cmpMax/\ptpKey{base/rpi-4/\cmpMaxArg/q50}} worse than the median baseline performance. While this drift is definitely noticeable, it is fortunately not even close to the theoretical worst-case window of uncertainty of \fTimeMS{\windowOfUncertaintyOneMinute}, representing just \fPercentage[2]{\cmpMax/\windowOfUncertaintyOneMinute} of the maximum software induced drift. We also observe that it is possible to get lucky: in some trials there is very little observed offset despite a full minute of downtime (just \fTime{\cmpMin} with \fVendor{\cmpMinArg}). In any case, all PTP implementations can quickly reconverge on the clock signal within a few seconds of restarting. Due to a quirk in how the PTP protocol works (slaves request synchronization signal leases from the master with a predetermined expiry), the master node will keep sending synchronization messages to the dead peer, which combined with the second lease that is issued when the PTP slave is restarted can help the peer synchronize quicker due to the higher influx of synchronization messages (this is sometimes referred to as an abandoned sync~\cite{sptp}).

In high reliability deployments, replication is frequently applied to increase dependability. However, in order for replication to be useful, we need to ensure that a failure can be contained to the node of origin. In terms of PTP, this means that the fault on one slave should not affect the quality of synchronization of the other slave. To verify that this is the case, we attached a second control node to the same master (Figure~\ref{fig:software_fault} right\todo{improve graphic}), which shows us that there is no meaningful degradation of the performance of other nodes.

One important caveat is to be noted for PTPd: when multiple software faults start occurring (not necessarily in close proximity to each other), problems can start propagating. On our test systems, we observe that upon exactly the third software fault, PTPd will cause the network interface to fail. In fact, no amount of manually bringing the network down and back up or reloading the network interface driver can solve this problem, up until the system is restarted by hand. This is especially critical as it implies that not only will the timing system fail, but it will simultaneously bring down any applications that need to communicate over the interface as well. Ironically, in this case, rather than support the deployment high-reliability distributed systems, PTPd will actually harm said reliability. We thus advise caution when deploying PTPd or any of its derivatives that may contain the same bug.

\subsection{Hardware Fault -- Slave}
\cmpSearchVendor{\ptpKey{fault/hardware/slave/rpi-4/\vendor/fault/post_max/max}}
\xdef\maxPiFour{\cmpMax}
\cmpSave{hw_slave_rpi-4}

\cmpSearchVendor{\ptpKey{fault/hardware/slave/rpi-5/\vendor/fault/post_max/max}}
\xdef\maxPiFive{\cmpMax}
\cmpSave{hw_slave_rpi-5}


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{res/generated/fault/hardware/slave_cluster_comparison.pdf}
    \legend
    \caption{A hardware fault on the slave of the Raspberry-Pi 4 cluster (left) and the Raspberry-Pi 5 cluster (right). Both slaves need to fully resynchronize after rebooting, but the Raspberry-Pi 5 has an advantage due to its hardware clock.}
    \label{fig:hardware_fault_slave}
\end{figure}

A slightly more difficult scenario is a hardware fault on the slave, as not only will the internal PTP state be lost, but also the kernel state containing the current system time and the clock drift. This implies that PTP will need to fully reconverge, using the standard step-and-slew approach seen in the baseline. Because the Raspberry-Pi 4 (Figure~\ref{fig:hardware_fault_slave} left) does not contain a real-time clock to preserve the system time across reboots, after a reboot a node will have a system time equal the last time that the time was persisted to disk if fake-hwclock~\cite{fake-hwclock-manpage} is active or a default value (e.g. the epoch in 1970) if fake-hwclock is not active. On our system (Raspbian, which bases on Debian), fake-hwclock is configured, so we observe a temporary deviation of just \cmpLoad{hw_slave_rpi-4}\fTimeMin{\cmpMax} (on \fVendor{\cmpMaxArg}). To fix the large offset, PTP needs to step the clock (breaking invariant I2) and then takes some minutes to reconverge on the previous level of accuracy (in the case of PTPd, the other vendors are generally faster). On the other hand, the Raspberry-Pi 5 can maintain the current time using the hardware clock while it is not running\todo{Fault not visible on RPI-5 chart}. While the RTC generally has lower resolution than the internal oscillator (a common resolution is 1/32768), it generally has good long-term stability\todo{cite}. This advantage shows itself in the observed maximum offset, which is a comparatively tiny \cmpLoad{hw_slave_rpi-5}\fTime{\cmpMax} (\fVendor{\cmpMaxArg}), a full \fNum{ln(\maxPiFour/\maxPiFive)/ln(10)} orders of magnitude smaller that can easily be compensated by clock slew only. A fault on a system without a hardware clock will thus cause an application to encounter large inconsistencies in timing in a place where it might not expect it (right in the middle of a run), while a simple RTC can mitigate this problem entirely. For deployments that are stuck on systems where an RTC is not viable (e.g. due to cost reasons), it is recommended to configure a delay for the application relaunch to allow PTP to resynchronize first.

\subsection{Hardware Fault -- Master}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{res/generated/fault/hardware/master_cluster_comparison.pdf}
    \legend
    \caption{A hardware fault on the master of the Raspberry-Pi 4 cluster (left) and the Raspberry-Pi 5 cluster (right). Slaves cannot compensate for unexpected changes in the announced time without correct configuration, so large clock offsets may be present practically indefinitely.}
    \label{fig:hardware_fault_master}
\end{figure}


PTP slaves are not the only type of node that can fail, the PTP master can just as well be affected by a disruption. Of all the scenarios, this is the most difficult one, as a failure will cause all slave nodes to become desynchronized. A failure on the master will inevitably also lead to hiccups on the announced time, especially in the embedded scenario where no external clock source for ground truth is available. On the Raspberry-Pi 4 (Figure~\ref{fig:hardware_fault_master} left\todo{Fix this chart too}), we observe that when the master restarts and now has a different reference time, there is once again a large offset between the master time and the slave time. However, unlike previously, the slave nodes are now no longer able to compensate for the offset by creating a clock step and instead are stuck at an (almost) constant offset to the master node. The reason for this lies in the default configuration of the PTP clients: It is assumed that stable time needs to be served to the application, so invariants I1 and I2 may not be broken, except once at start-up time when a single clock step is permissible to avoid excessive convergence time through clock slew. Because the slaves did not restart, they are not allowed to perform a clock-step, and while they can easily observe the difference they are stuck converging on it with a regular clock slew, which for the observed offset of \fTimeMin{\maxPiFour} is projected to require at least \fTimeMin[-3]{\maxPiFour/\maxClockSlew} -- clearly impractical. Moreover, while the slaves will be converging on the new master clock at close to the maximum software drift rate simultaneously, we observe that they do not stay particularly well in sync between each other while this happens because the hardware clock drift continues to differ between nodes, which means that the remaining peers can also not rely on their clocks being close to each other either. This problem of indefinite clock-slew can be avoided by reconfiguring the PTP slave to also allow subsequent clock steps (which is disabled by default for safety), but the system should be well tested for resilience as the application will have to cope with a large magnitude rewinding of the clock (this is the worst case, breaking both I1 and I2). Experimentation on the stress-test tools and other applications show that many misbehave during a clock step, so it cannot be assumed that an application is resilient simply because it uses the monotonic clock.

As already observed previously, the issue is easily mitigated by using an external clock that can maintain system time during downtime (e.g. Raspberry-Pi 5). The clock source does not need to be of particularly high quality if the goal is to only maintain sub-second synchronization accuracy. Should constraints only allow an external clock to be installed on a single node, intuition and our observations suggest that the primary target for installation should be the master node -- it can propagate its stable time source to the slaves.

\subsection{Hardware Fault -- Failover Master}

\begin{figure}
    \includegraphics[width=\linewidth]{res/generated/fault/hardware/master_failover_cluster_comparison.pdf}
    \caption{Fault Tolerance with a Failover Master}
    \label{fig:failover}
\end{figure}

To increase service availability it is common to install a failover for any single point of failure in a distributed system so that the backup system can take over when the primary system fails. This is not only applicable for data-centers, but also applies to our minimal three node embedded setup, as a failover master will allow the two remaining nodes to remain synchronized with each other when the primary master fails. Fortunately, PTP was designed with this explicitly in mind so that the failover master does not necessarily require its own clock source, the failover node can automatically assume a slave state while the master is healthy (thus acquiring a clock signal consistent with the master's) and subsequently promote itself to a master when the original master is no longer healthy. Note that this setup is not supported in Meta's SPTP, as masters and slaves run different binaries and thus cannot switch roles (an external clock source would be necessary for two synchronized masters, we thus exclude SPTP from this evaluation).

We observe that the failover master can very rapidly take over the master role in the event of a failure, causing virtually no disruption in the timing service (Figure~\ref{fig:failover}\todo{data import failed}). However, the presence of a failover master cannot mitigate the problem of the master eventually restarting and disseminating a different time, which will be picked up by the failover master and the slaves, who will proceed to again get stuck in an indefinite clock slew. Due to the way the best master clock algorithm works (the same master clock will always be selected unless a configuration is changed thus shifting the prioritization), the failover master cannot re-teach the current network time to the master that has just restarted because it does not take priority over the actual master unless the master is demoted through configuration (e.g. using data from an external clock source).

\subsection{Network Fault}

Another possible root cause for a fault is the network. While a network outage is generally bad news for reliable distributed systems that need to communicate, a total network outage is ironically easier for PTP to handle than a functioning network with high levels of congestion or a contained failure within any of the nodes. Since no state is lost, the servos are kept in holdover and PTP just has to detect when the network is available again to reconverge on the common network time. The effects of this scenario are not particularly surprising, so we do not show it separately\todo{include perhaps 1 number}.


\section{Resource Consumption}
\label{sec:resource_consumption}
\todo{Find where to place this section.}

\begin{figure*}
\centering
\includegraphics[width=\linewidth]{res/generated/resource_consumption/summary.pdf}
\legend
\caption{Resource consumption statistics across vendors and systems. In general, PTP systems use few onboard resources but generate more network traffic, while the evaluated NTP and SPTP implementations reduce network utilization at the cost of additional ROM and other on-board resources.}
\label{fig:resource_consumption}
\end{figure*}

Finally, embedded systems need to take particular care of the resource consumption of their applications. Since time synchronization fulfills only a supporting role in the overall deployment, it is important that it does not detract significantly from the resources available for the actual application. This is not only true for resources with a hard limit (compute, memory, etc.) but also for ``soft'' resources like power draw and heat dissipation. Especially SPTP was created with the intention of less resource consumption in mind~\cite{sptp}, so we will verify whether reality holds up to these claims.

While some PTP systems are specifically tied to Linux and thus come with the associated resource requirements, others can also be deployed in more lightweight environments. On low capability devices, ROM/flash space is generally restricted, so the footprint becomes a concern. PTPd has the comparatively lightest footprint, with executables, data and dependencies totaling at around 840\,KB after removal of pre-packaged documentation and dependencies that can reasonably be expected to already be available (e.g. lib-c). The LinuxPTP package is larger but requires fewer dependencies, thus the resulting footprint of 970\,KB is around the same. SPTP (21\,MB) and Chrony (12\,MB) are both more than an order of magnitude larger due to the integrated Go runtime and the comparatively large number of dependencies, respectively. Note that the size of SPTP could be reduced by around 40\% by only deploying the master or client executable and the Chrony footprint is significantly smaller if the larger dependencies (iproute2 + libgnutls30 + tzdata $=$ 10\,MB) are already available. In combination with an embedded Linux distribution, the more lightweight systems could reasonably be deployed on 16\,MB flash devices, although this would likely not leave sufficient headroom for the actual application -- thus 32\,MB is the more realistic lower bound for PTP deployments.

Closely tied to the storage footprint is the memory footprint. LinuxPTP (250\,KB-400\,KB), Chrony (800\~KB-1\,MB) and PTPD ($\sim$1\,MB) all have reasonably small unique set sizes, with resident set sizes around 2-4\,MB. SPTP requires significantly more, with unique and resident set ranging between 8-10\,MB for the master node and 15-16\,MB on the slaves. Also notable is the virtual memory allocated by SPTP, 3\,GB on the master and 1\,GB on the client: over 100x more space than the next most hungry implementation (Chrony at 11\,MB) and enough to cause potential issues on 16-bit deployments and platforms that don't have large amounts of virtual memory available.

Aside from deployability, power consumption is another big concern especially for mobile deployments. Using consumed CPU time as a heuristic for power consumption, we observe that LinuxPTP and PTPd consume the fewest compute, while Chrony requires around 30\%$-$150\% more. SPTP consumes the most cycles, with around 300\% more consumption than LinuxPTP and PTPd, causing the system to run 0.3C$-$0.6C hotter on both the Raspberry-Pi 4 cluster and the Raspberry-Pi 5 cluster (indicative of the additional energy expended). Overall, the consumption of compute power is however relatively small even for SPTP, with around 10\,s of compute consumed in 1 hour of runtime on our comparatively powerful ARM Cortex-A72.

Network traffic can also be a concern for low-throughput networks. Previous studies have found that PTP tends to consume a negligible amount of bandwidth\todo{cite}. Our results suggest that SPTP consumes the fewest network resources overall (around 1.1\,MB per hour for one client), only surpassed by the Chrony master endpoint, which uses even less. The Chrony slaves consume more traffic, up to double the data rate. So while Chrony might have a better total network traffic score, there is a skew between the master and the slaves. PTPd and LinuxPTP are roughly equal on both data and packet rates since they implement the same protocol, totaling around 80\% above SPTP's resource consumption. Thus, the claim that traffic can be reduced by simplifying the PTP protocol~\cite{sptp} using SPTP appears valid, however the NTP client Chrony shows roughly comparable efficiency using a well established protocol.

Overall, despite SPTP's promise of better resource efficiency, we find that SPTP surpasses the other vendors only in network traffic efficiency (likely due to the simplified protocol), but uses considerably more ROM, RAM, and compute (Figure~\ref{fig:resource_consumption}\todo{check whether this can be converted into a sneaky scalability analysis for resource consumption between 2-6 peers.}). Thus, it is actually less well suited to embedded deployments than the more widespread PTP vendors. However, this does not necessarily contradict the claims made in the SPTP article~\cite{sptp}, since the article targeted mass-scale deployments of 100K clients, a scale that is highly uncommon in the embedded world. For minimizing resource consumption on embedded systems, we recommend LinuxPTP foremost: it has the highest efficiency in on-board resource consumption at the cost of comparatively high network traffic, but it outperforms the other suitable vendor PTPd in synchronization quality.
