\newcommand{\safetyMargin}{1000$\times$}

\section{Failure Scenarios}
\label{sec:failure_scenarios}
\todo{Should this be a sub-section}

Wherever accurate clock synchronization is more than just a nice-to-have, and applications need to rely on the accuracy of the clock to ensure correctness, we need better guarantees than just an estimate of the accuracy.
Ideally, the underlying technology provides a bounded latency guarantee, but current commodity hardware and software based on Ethernet does not yet offer this.
Still, resilience is a top priority when building dependable systems, and to ensure that a system can operate reliably engineers have to rely on safety margins where guarantees are not available to prevent failures from propagating.
In the context of time synchronization, since estimating the offset between clocks in software is imprecise even after applying denoising techniques, and the actual offset is influenced by a range of environmental conditions, the margin needs to be sufficiently large.
The high level of uncertainty combined with risk of major consequences of failure, and/or system criticality for certain application scenarios can merit safety factors of 5$\times$, 10$\times$ or beyond depending on the application scenario\todo{Would be nice to have something to cite here}.
Naturally, the exact factor varies between use-cases, but for the purpose of our analysis of determining what conditions can lead to failures, we show that even highly conservative safety margins (e.g. three orders of magnitude, \safetyMargin) over the baseline can be broken, reinforcing the necessity of careful consideration of possible causes of failure.
%Thus, for our testbed of Raspberry Pis, a failure constitutes any time when the measured clock offset exceeds 40\,\textmu{}s\todo{out-of-place: hardware not introduced.}\todo{Actually insert the baseline.}.

\subsection{Sources of Error}
\todo{Should this be listed as a contribution?}
Error can originate from 2 principal sources: either the clock fails to perform according to specifications, or PTP's ability to measure and compensate the difference between clock sources is degraded or disrupted entirely. To understand what constitutes a failure in a timing system, we need to consider the guarantees that a timing system is required to provide to be useful and how they might be broken.

\paragraph*{Invariants of Timing Sources} Many of the assumptions made when using a clock are made implicitly because they seem natural in the real world, and a surprising number of software systems rely on them without being specifically aware of it. Timing systems need to perpetually fulfill all the following invariants, ranked in decreasing order of importance:

\newcounter{errorConditions}
\begin{enumerate}[label=I\arabic*.]
    \item \textbf{Time flows forward.}\todo{@Arpan wants citations but as of now: Based on my understanding. Find if somebody has formalized this.} Because it seems a trivial assumption, it has the worst consequences when it is violated. Many software systems do not account for the possibility of time flowing backwards or seeing timestamps from the future, and this can cause common cascading failures including missed deadlines (in cyclic events), data inconsistencies (through tasks being executed multiple times), and data loss (accidental overwriting of previously collected data/backups based on timestamps). Due to the risk, even minor backward time shifts are dangerous for system stability.
    \item \textbf{Time passes continuously, it does not jump.} While systems expect time to pass, jumping too far into the future can result in load spikes through e.g. too many scheduled tasks arriving simultaneously or scheduled tasks being skipped entirely. For stability, direct modifications of a clock at system runtime should be avoided in order to avoid breaking any real-time constraints the system is trying to observe.
    \item \textbf{Time passes at a constant rate.} While it seems like the most fundamental constraint, I3 is actually the easiest to bend without compromising usefulness. No process is perfect, and this implies a certain tolerance of clock drift (usually specified in the data sheet) is necessary. In fact, software is designed to cope with additional variance in the flow of time, as modern day systems are often too complex to schedule anything at a precise point in time. Making clocks tick slightly faster or slower can be used for synchronization with fewer stability implications, which is leveraged in the clock slew.
    \setcounter{errorConditions}{\value{enumi}}
\end{enumerate}

Even during our experiments, which were carefully designed to account for the unreliable telling of time, we ran into issues with all of the above since PTP inevitably needs to violate (potentially all of) these assumptions to synchronize the clocks when there is a difference. Correcting for this error is difficult when there is no second source of time to cross-validate with, which is why we eventually used an external time source for record-keeping. Generally, PTP implementations try to minimize disruptions by breaking the lowest priority invariant possible to keep clocks in sync, exploiting the fact that I3 is more of a soft requirement.

\paragraph*{Multiple Sources of Time}

When two or more clocks are at play, then another invariant arises:

\begin{enumerate}[label=I\arabic*.]
    \setcounter{enumi}{\value{errorConditions}}
    \item \textbf{Coherence between ordered readings of different clocks.} To leverage clocks for e.g. transaction ordering we need to ensure that a reading of one clock that comes before another should always yield a timestamp smaller than or equal to the latter to allow coherence comparisons. This is theoretically equivalent with obtaining the same time when reading both clocks simultaneously assuming arbitrary precision, but this is not possible in practice. Since there is always a margin of error, the coherency is only considered to be violated when the order is not preserved across a tolerance bound that the application relies on.
\end{enumerate}

Establishing and maintaining coherency is PTP's primary purpose, and maintaining invariant I4 does not have a fixed priority\todo{Find if somebody has formalized this}, it sometimes merits breaking I3, I2 or even I1. However, for synchronization to even become possible two preconditions need to be fulfilled. These are the two principal challenges that PTP has to solve:

\begin{enumerate}[label=P\arabic*.]
%    \setcounter{enumi}{\value{errorConditions}}

    \item \textbf{Clock difference can be measured.} The prerequisite to synchronizing two clocks is to be able to quantify the offset, which is difficult to do reliably across packet-switched networks with best-effort quality of service due to the inherent packet-delay variation present.
    \item \textbf{Clock difference can be corrected.} Even when a difference can be measured it does not automatically imply that it can be corrected. Modifying current time has significant repercussions, so time synchronization programs have rules in place limiting under what conditions each invariant I1-I3 may be violated.
\end{enumerate}

In order for two clocks to stay synchronized, both conditions need to be continuously fulfilled to prevent divergence from occurring, which would cause invariant I4 to break down.

\subsection{Consequences of Failing to Maintain Invariants}
A violation of any invariant is an error in the timing system that can propagate throughout the entire system.
I1-I3 failures will cause local incoherence, potentially cascading locally or indirectly across a network. On the other hand, problems with time synchronization prerequisites cause clocks to diverge (I4) and thus coherency across the distributed system breaks down, which impacts the distributed application but can also propagate back into local components.
While coherency might seem less important, PTP's central obligation is to provide reliable and high accuracy time synchronization so that algorithms can use time for distributed scheduling, so a failure to provide this invalidates the usefulness of PTP itself.

The importance of maintaining clock invariants can be easy to underestimate. While small errors seem negligible for most timekeeping use cases, multiple production outages have been linked to timekeeping failures.
%One common cause is the existence of the leap-second, which,
%, an extra second that occurs once every few years to keep the calendar in sync with the earth's rotation.
The leap-second, despite its small magnitude, has been known to cause problems with navigation/collision avoidance systems, dead/live-locks occurring in the Linux kernel, and multi-hour outages in financial transaction processing systems~\cite{leap-seconds-recap}.
Significant engineering work has gone into mitigations, e.g. employing a leap-smear~\cite{leap-second-google, leap-second-technical-aspects} to avoid a time jump (rationale: condition I3 is less important than I1/I2), but these have in turn caused other outages,
%, but imperfections in the mitigation systems have in turn caused synchronization failures leading to accidental time jumps of 10 years magnitude~\cite{leap-seconds-recap}, large enough to break even ``simple'' systems that we take for granted such as SSL.
%The leap-second is such a significant reliability problem that recently calls are being made to abolish it altogether rather than trying to deal with its consequences~\cite{leap-second-facebook-abolish}, a resolution which has since found its way into the definition of UTC~\cite{leap-second-resolution}.
%Thus, the leap-second was
eventually leading to complete abolishment of leap-seconds via an amendment to UTC~\cite{leap-second-resolution,leap-second-facebook-abolish}.
When even well-vetted software such as the Linux kernel has resilience issues with one-second breaks in timing invariants, one can expect the average user application to be much less robust still, and no amount of replication can mitigate this.

Throughout this study, we will determine what can be expected of network time protocols, when and under what conditions the invariants and prerequisites inevitably break down under adverse conditions, and what this means for the ability of applications to reliably leverage Ethernet-based time synchronization.
%\todo{Explicitly list what our 10x bound means for each invariant.}

%\begin{table}
%    \centering
%    \caption{Conditions that constitute a violation of the time source invariants, as per the defined \safetyMargin{} safety margin above the baseline.}
%    \begin{tabular}{crr}
%        Invariant & Raspberry-Pi& 2nd System\\
%        I1 & $\Delta{}t<0$& \\
%        I2& $\Delta{}t>40$\textmu{}s& \\
%        I3& --& \\
%        I4& $|t_{C1} - t_{C2}| > 40$\textmu{}s& \\
%    \end{tabular}
%    \label{tbl:invariant-violation-limits}
%\end{table}
